{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons with competitor algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmentr Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "from matplotlib.ticker import MaxNLocator  # type: ignore\n",
    "\n",
    "from sklearn.manifold import TSNE  # type: ignore\n",
    "from sklearn.decomposition import PCA  # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.linear_model import LinearRegression  # type: ignore\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "dataset_params = {\n",
    "        \"SIFT100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"lightblue\",\n",
    "            \"marker\": \"o\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"SIFT100M\"\n",
    "        },\n",
    "        \"DEEP100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 750,\n",
    "            \"color\": \"plum\",\n",
    "            \"marker\": \"x\",  \n",
    "            \"li\": 1 ,\n",
    "            \"label\": \"DEEP100M\"\n",
    "        },\n",
    "        \"T2I100M\": {\n",
    "            \"M\": 80,\n",
    "            \"efC\": 1000,\n",
    "            \"efS\": 2500,\n",
    "            \"color\": \"rosybrown\",\n",
    "            \"marker\": \"d\",\n",
    "            \"li\": 2,\n",
    "            \"label\": \"T2I100M\"\n",
    "        },\n",
    "        \"GLOVE100\": {\n",
    "            \"M\": 16,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"orange\",\n",
    "            \"marker\": \"^\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GLOVE1M\"\n",
    "        },\n",
    "        \"GIST1M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 1000,\n",
    "            \"color\": \"lightgreen\",\n",
    "            \"marker\": \"s\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GIST1M\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "SEED = 42\n",
    "s = 1000\n",
    "\n",
    "def get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi):\n",
    "    return f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{float(r_target):.2f}_ipi{ipi}_mpi{mpi}.txt\"\n",
    "\n",
    "\n",
    "def get_detailed_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi):\n",
    "    return f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/detailed/M{M}_efC{efC}_efS{efS}_qs{s}_tr{float(r_target):.2f}_ipi{ipi}_mpi{mpi}.txt\"\n",
    "\n",
    "\n",
    "def get_naive_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target):\n",
    "    return f\"../../experiments/results/naive-early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{float(r_target):.2f}.txt\"\n",
    "\n",
    "\n",
    "def get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k):\n",
    "    return f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\"\n",
    "\n",
    "\n",
    "def get_laet_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target):\n",
    "    return f\"../../experiments/results/laet-early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{float(r_target):.2f}.txt\"\n",
    "\n",
    "\n",
    "def get_classic_hnsw_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target):\n",
    "    return f\"../../experiments/results/classic-hnsw/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{float(r_target):.2f}.txt\"\n",
    "\n",
    "\n",
    "def get_optimal_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k):\n",
    "    return f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/detailed.M{M}_efC{efC}_efS{efS}_qs{s}.txt\"\n",
    "\n",
    "\n",
    "def get_heuristic_interval_conf_filename(version=\"v1\"):\n",
    "    return f\"../../experiments/generated_json/heuristic_recommendations_params_{version}.json\"\n",
    "\n",
    "\n",
    "def get_testing_detailed_dataset_name(M, efC, efS, s, ds_name, k, logint): \n",
    "    return f\"../../experiments/results/test_logging/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_li{logint}.txt\"\n",
    "\n",
    "\n",
    "def get_model_name(M, ef, s, ds_name, k, logint, selected_features, n_estimators=100):\n",
    "    return f\"../../predictor_models/lightgbm/{ds_name}_M{M}_ef{ef}_s{s}_k{k}_nestim{n_estimators}_li{logint}_{selected_features}.txt\"\n",
    "\n",
    "\n",
    "def compute_P99(y_true, y_pred):\n",
    "    y_diff = np.abs(y_true - y_pred)\n",
    "    return np.percentile(y_diff, 99)\n",
    "\n",
    "# Load the dataconf from a file:\n",
    "interval_conf = {}\n",
    "with open(\"../../experiments/generated_json/final_heuristic_adaptive_recommendations_params_ipidiv2_mpidiv10.json\") as f:\n",
    "    interval_conf = json.load(f)\n",
    "\n",
    "print(interval_conf.keys(), interval_conf[\"SIFT100M\"].keys(), interval_conf[\"SIFT100M\"][\"100\"].keys())\n",
    "\n",
    "# Load the laet conf\n",
    "laet_multiplier_conf = {}\n",
    "with open(\"../../experiments/generated_json/laet_tuning_results_memoryFalse_validationSize1000.json\") as f:\n",
    "    laet_multiplier_conf = json.load(f)\n",
    "\n",
    "print(laet_multiplier_conf.keys(), laet_multiplier_conf[\"SIFT100M\"].keys(), laet_multiplier_conf[\"SIFT100M\"][\"100\"].keys(), laet_multiplier_conf[\"SIFT100M\"][\"100\"][\"0.8\"])\n",
    "\n",
    "# Load the classic hnsw/REM conf\n",
    "classic_hnsw_conf = {}\n",
    "with open(\"../../experiments/generated_json/classic_hnsw_tuning_results_memoryFalse_validationSize1000.json\") as f:\n",
    "    classic_hnsw_conf = json.load(f)\n",
    "\n",
    "print(classic_hnsw_conf.keys(), classic_hnsw_conf[\"SIFT100M\"].keys(), classic_hnsw_conf[\"SIFT100M\"][\"100\"].keys(), classic_hnsw_conf[\"SIFT100M\"][\"100\"][\"0.8\"])\n",
    "\n",
    "algorithm_conf = {\n",
    "    \"Naive-Baseline\": {\n",
    "        \"color\": \"black\",\n",
    "        \"marker\": \"s\",\n",
    "        \"label\": \"Baseline\",\n",
    "    },\n",
    "    \"LAET\": {\n",
    "        \"color\": \"gray\",\n",
    "        \"marker\": \"x\",\n",
    "        \"label\": \"LAET\",\n",
    "    },\n",
    "    \"HNSW\": {\n",
    "        \"color\": \"lightgray\",\n",
    "        \"marker\": \"^\",\n",
    "        \"label\": \"REM\",\n",
    "    },\n",
    "    \"DARTH\": {\n",
    "        \"color\": \"tomato\",\n",
    "        \"marker\": \"o\",\n",
    "        \"label\": \"DARTH\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PLOTS_DIR = \"./../../experiments/revision-plots/\"\n",
    "\n",
    "algorithm_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning overhead experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "\n",
    "def format_large_values(value, _):\n",
    "    \"\"\"Formats large values as K for thousands.\"\"\"\n",
    "    if value >= 1_000:\n",
    "        return f\"{value / 1_000:.1f}K\" if value % 1_000 != 0 else f\"{int(value / 1_000)}K\"\n",
    "    return str(int(value))\n",
    "\n",
    "for k in all_k_values:\n",
    "    all_queries_before_laet = [] # for all datasets to extract statistics\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    width = 0.20\n",
    "    x = np.arange(len(all_r_targets))\n",
    "    \n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        \n",
    "        if ds_name == \"T2I100M\":\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\"]\n",
    "        else:\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "        \n",
    "        laet_vals = []\n",
    "\n",
    "        for r_target in all_r_targets:\n",
    "            time_to_tune_LAET_ms = laet_multiplier_conf[ds_name][k][r_target][\"total_tuning_time\"]\n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k))\n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= float(r_target)]                \n",
    "        \n",
    "            ipi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            darth_df = darth_df[darth_df[\"r_actual\"] >= float(r_target)]\n",
    "            \n",
    "            avg_time = darth_df[\"elaps_ms\"].mean()\n",
    "            queries_before_laet = time_to_tune_LAET_ms / avg_time\n",
    "\n",
    "            laet_vals.append(queries_before_laet)\n",
    "            all_queries_before_laet.append(queries_before_laet)\n",
    "        \n",
    "        offset = (i - len(all_datasets) / 2) * width + width / 2\n",
    "        ax.bar(x + offset, laet_vals, width, label=ds_name, color=dataset_params[ds_name][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_r_targets)\n",
    "    ax.set_xlabel(r\"$R_{t}$\")\n",
    "    ax.set_ylabel(\"# Queries\")\n",
    "    \n",
    "    y_labels = [0, 5000, 10000]\n",
    "    ax.set_yticks(y_labels)   \n",
    "    \n",
    "    # set limit for y axis to be at 10000\n",
    "    ax.set_ylim(0, 14000) \n",
    "    \n",
    "    \n",
    "    x_labels = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    \n",
    "    # put 2-2 orientation for legend:\n",
    "    legend_handles = [\n",
    "        #Patch(facecolor=dataset_params[ds_name][\"color\"], label=ds_name) for ds_name in all_datasets\n",
    "        # Replace GLOVE100 name with GLOVE1M\n",
    "        Patch(facecolor=dataset_params[\"GLOVE100\"][\"color\"], label=\"GLOVE1M\") if ds_name == \"GLOVE100\" else Patch(facecolor=dataset_params[ds_name][\"color\"], label=ds_name) for ds_name in all_datasets\n",
    "    ]\n",
    "    ax.legend(handles=legend_handles, fontsize=32, ncol=2, handletextpad=0.1, handlelength=1.2, columnspacing=0.1, frameon=False, bbox_to_anchor=(0.5, 1), loc='center')\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.8)\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(format_large_values))\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    #ax.legend()\n",
    "    ax.tick_params(axis='x', labelsize=38)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    filename = f\"{PLOTS_DIR}tuning_overheads_LAET_k{k}.pdf\"\n",
    "    fig.savefig(filename, bbox_inches=\"tight\")\n",
    "    print(f\"Saved plot at {filename}\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"{k}: Average queries before LAET: {round(np.mean(all_queries_before_laet))}\")\n",
    "    print(f\"{k}: Median queries before LAET: {round(np.median(all_queries_before_laet))}\")\n",
    "    print(f\"{k}: Max queries before LAET: {round(np.max(all_queries_before_laet))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Competitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legend Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 48})\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))  # Adjust the figsize for better alignment\n",
    "legend_elements = [\n",
    "    Patch(facecolor=algorithm_conf[alg][\"color\"], edgecolor=algorithm_conf[alg][\"color\"], alpha=0.8, label=algorithm_conf[alg][\"label\"]) for alg in algorithm_conf.keys()\n",
    "]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False)\n",
    "ax_legend.axis('off')\n",
    "savefile = f\"{PLOTS_DIR}competitors_bars_legend.pdf.pdf\"\n",
    "fig_legend.savefig(savefile, bbox_inches=\"tight\")\n",
    "print(\"Saved legend at \", savefile)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 48})\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))  # Adjust the figsize for better alignment\n",
    "legend_elements = [\n",
    "    #Patch(facecolor=algorithm_conf[alg][\"color\"], edgecolor=algorithm_conf[alg][\"color\"], alpha=0.8, label=algorithm_conf[alg][\"label\"]) for alg in algorithm_conf.keys()\n",
    "    Line2D([0], [0], color=algorithm_conf[alg][\"color\"], marker=algorithm_conf[alg][\"marker\"], markersize=30, linewidth=4, label=algorithm_conf[alg][\"label\"]) for alg in algorithm_conf.keys()\n",
    "]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False)\n",
    "ax_legend.axis('off')\n",
    "savefile = f\"{PLOTS_DIR}competitors_lines_legend.pdf\"\n",
    "fig_legend.savefig(f\"{savefile}\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved legend at \", savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 48})\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))  # Adjust the figsize for better alignment\n",
    "legend_elements = [\n",
    "    Patch(facecolor=algorithm_conf[alg][\"color\"], edgecolor=algorithm_conf[alg][\"color\"], alpha=0.8, label=algorithm_conf[alg][\"label\"]) for alg in [\"HNSW\", \"DARTH\"]\n",
    "]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False)\n",
    "ax_legend.axis('off')\n",
    "savefile = f\"{PLOTS_DIR}hnsw_darth_only_competitors_bars_legend.pdf\"\n",
    "fig_legend.savefig(savefile, bbox_inches=\"tight\")\n",
    "print(f\"Saved legend at {savefile}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unified_legend(algorithm_conf):\n",
    "    \"\"\"\n",
    "    Generates a unified legend for bar and line plots.\n",
    "\n",
    "    Args:\n",
    "        algorithm_conf: A dictionary containing the following keys for each algorithm:\n",
    "            - \"color\": Color of the bar or line.\n",
    "            - \"marker\": Marker style for line plots (e.g., 'o', 's', 'x').\n",
    "            - \"label\": Label for the algorithm.\n",
    "\n",
    "    Returns:\n",
    "        fig_legend: Figure object containing the legend.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.rcParams.update({\"font.size\": 48})\n",
    "\n",
    "    fig_legend, ax_legend = plt.subplots(figsize=(8, 1))\n",
    "\n",
    "    legend_elements = []\n",
    "    for alg in algorithm_conf.keys():\n",
    "        legend_elements.append(\n",
    "            Patch(facecolor=algorithm_conf[alg][\"color\"], \n",
    "                  edgecolor=algorithm_conf[alg][\"color\"], \n",
    "                  alpha=0.8, \n",
    "                  label=f\"{algorithm_conf[alg]['label']}\"))\n",
    "        \n",
    "        legend_elements.append(\n",
    "            Line2D([0], [0], \n",
    "                   color=algorithm_conf[alg][\"color\"], \n",
    "                   marker=algorithm_conf[alg][\"marker\"], \n",
    "                   markersize=30, \n",
    "                   linewidth=2, \n",
    "                   label=f\"{algorithm_conf[alg]['label']}\")\n",
    "            )\n",
    "        \n",
    "\n",
    "    ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False)\n",
    "    ax_legend.axis('off')\n",
    "\n",
    "    return fig_legend\n",
    "\n",
    "fig_legend = create_unified_legend(algorithm_conf)\n",
    "fig_legend.savefig(f\"{PLOTS_DIR}/unified_competitor_legend.pdf\", bbox_inches=\"tight\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Distribution Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"DEEP100M\", \"SIFT100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]#\n",
    "all_r_targets = [\"0.95\"]\n",
    "\n",
    "versus_algos = [\"Naive-Baseline\", \"LAET\", \"HNSW\"] # or LAET, Baseline]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "bar_width = 0.2\n",
    "s = 1000\n",
    "\n",
    "for i, ds_name in enumerate(all_datasets):\n",
    "    M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "    for r_target in all_r_targets:\n",
    "        for k in all_k_values:\n",
    "            ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "            f_r_target = float(r_target)\n",
    "                    \n",
    "            no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]              \n",
    "                    \n",
    "            cefs = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "            hnsw_df = pd.read_csv(get_classic_hnsw_early_stop_testing_dataset_name(M, efC, classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"], s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            naive_df = pd.read_csv(get_naive_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            laet_df = pd.read_csv(get_laet_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            \n",
    "            comp_versus = {\n",
    "                \"LAET\": laet_df,\n",
    "                \"HNSW\": hnsw_df,\n",
    "                \"Naive-Baseline\": naive_df,\n",
    "            }\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10,6))\n",
    "            \n",
    "            for versus_algo in versus_algos:\n",
    "                competitor_df = comp_versus[versus_algo]\n",
    "                competitor_df = competitor_df[competitor_df[\"qid\"].isin(no_early_stop_df[\"qid\"])] \n",
    "                recall_competitor = competitor_df[\"r\"].mean()\n",
    "                competitor_rqt = competitor_df[competitor_df[\"r\"] < f_r_target].shape[0]\n",
    "                ax.hist(competitor_df[\"r\"], alpha=0.8, label=\"\", color=algorithm_conf[versus_algo][\"color\"])#, bins=20)\n",
    "                print(f\"avg recall {versus_algo}: {recall_competitor} | rqt {versus_algo}: {competitor_rqt} | min recall {versus_algo}: {competitor_df['r'].min()}\")            \n",
    "            \n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "            recall_darth = darth_df[\"r_actual\"].mean()\n",
    "            speedup_vs_no_early_stop = no_early_stop_df[\"elaps_ms\"].mean() / darth_df[\"elaps_ms\"].mean()\n",
    "            darth_rqt = darth_df[darth_df[\"r_actual\"] < f_r_target].shape[0]\n",
    "            print(f\"avg recall DARTH: {recall_darth} | rqt DARTH: {darth_rqt} | Min recall DARTH: {darth_df['r_actual'].min()}\")\n",
    "            \n",
    "            ax.hist(darth_df[\"r_actual\"], alpha=0.8, label=\"\", color=algorithm_conf[\"DARTH\"][\"color\"], bins=6)\n",
    "            rt_str = r\"$R_{t}$\"\n",
    "            ax.axvline(x=f_r_target, color=\"red\", linestyle=\"--\", label=f\"{rt_str}={f_r_target}\", linewidth=3)\n",
    "            ax.set_xlabel(\"Actual Recall\")\n",
    "            ax.set_ylabel(\"# Queries (log)\", fontsize=34)\n",
    "            ax.grid(alpha=0.8, linestyle=\"--\") \n",
    "            ax.legend(fontsize=38, frameon=False, handletextpad=0.1, handlelength=1.0, columnspacing=0.3)   \n",
    "            ax.set_yscale(\"log\")     \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            fig.tight_layout()\n",
    "            savefile = f\"{PLOTS_DIR}competitor_histogram_{ds_name}_k{k}_rt{r_target}.pdf\"\n",
    "            fig.savefig(savefile, format=\"pdf\", bbox_inches = 'tight')\n",
    "            print(f\"Saved plot at {savefile}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Approximate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rqut_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    recalls = df[recall_col].round(2)\n",
    "    rqut = len(recalls[recalls < r_target]) / s\n",
    "    return rqut\n",
    "    \n",
    "def rde_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"RDE\"].mean()\n",
    "\n",
    "def tde_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"TDR\"].mean()\n",
    "\n",
    "def nrs_rev_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return 1/df[\"NRS\"].mean()\n",
    "\n",
    "def time_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"elaps_ms\"].mean()\n",
    "\n",
    "def dist_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"dists\"].mean()\n",
    "\n",
    "def p99_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] < r_target]\n",
    "    error = np.abs(below_df[recall_col] - r_target)\n",
    "    return np.percentile(error, 99)\n",
    "\n",
    "def error_mean_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] < r_target]    \n",
    "    return below_df[recall_col].mean()\n",
    "\n",
    "def worst_errors_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] < r_target]    \n",
    "    return below_df[recall_col].max()\n",
    "\n",
    "def worst_1perc_errors_mean_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    # Filter entries below the target recall\n",
    "    below_df = df[df[recall_col] < r_target].copy()\n",
    "    \n",
    "    # Calculate the error as the absolute difference\n",
    "    below_df[\"error\"] = abs(r_target - below_df[recall_col])\n",
    "    \n",
    "    # Sort the DataFrame by the error in descending order\n",
    "    sorted_below_df = below_df.sort_values(by=\"error\", ascending=False)\n",
    "    \n",
    "    # Calculate the top 1% of the worst errors\n",
    "    num_entries = len(sorted_below_df)\n",
    "    worst_1_percent_count = max(1, int(0.01 * num_entries))  # At least 1 entry\n",
    "    \n",
    "    # Select the worst 1% errors and calculate their mean\n",
    "    worst_1_perc_mean = sorted_below_df[\"error\"].head(worst_1_percent_count).mean()\n",
    "    \n",
    "    return worst_1_perc_mean\n",
    "\n",
    "def qps_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    average_time_ms = df[\"elaps_ms\"].mean()\n",
    "    average_time_seconds = average_time_ms / 1000\n",
    "    qps = 1 / average_time_seconds\n",
    "    return qps\n",
    "    \n",
    "quality_measures = {\n",
    "    \"RQUT\": {\n",
    "        \"label\": \"RQUT\",\n",
    "        \"calculator\": rqut_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"RDE\": {\n",
    "        \"label\": \"RDE\",\n",
    "        \"calculator\": rde_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"TDR\": {\n",
    "        \"label\": \"TDR\",\n",
    "        \"calculator\": tde_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"NRS_rev\": {\n",
    "        \"label\": r\"$NRS^{-1}$\",\n",
    "        \"calculator\": nrs_rev_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"Time\": {\n",
    "        \"label\": \"Time (ms)\",\n",
    "        \"calculator\": time_calculator,\n",
    "        \"type\": \"line\"\n",
    "    },\n",
    "    \"Dist\": {\n",
    "        \"label\": \"Dists\",\n",
    "        \"calculator\": dist_calculation,\n",
    "        \"type\": \"line\"\n",
    "    },\n",
    "    \"p99\": {\n",
    "        \"label\": r\"$P99$\",\n",
    "        \"calculator\": p99_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    #\"ErrorMean\": {\n",
    "    #    \"label\": \"Error Mean\",\n",
    "    #    \"calculator\": error_mean_calculation,\n",
    "    #    \"type\": \"bar\"\n",
    "    #},\n",
    "    #\"WorstErrors\": {\n",
    "    #    \"label\": \"Worst Errors\",\n",
    "    #    \"calculator\": worst_errors_calculation,\n",
    "    #    \"type\": \"bar\"\n",
    "    #},\n",
    "    \"Worst1perc\": {\n",
    "        \"label\": \"Worst 1%\",\n",
    "        \"calculator\": worst_1perc_errors_mean_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"QPS\": {\n",
    "        \"label\": \"QPS\",\n",
    "        \"calculator\": qps_calculator,\n",
    "        \"type\": \"line\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "all_measures = [\"Worst1perc\"]#, \"RDE\", \"p99\", \"RQUT\", \"NRS_rev\"]#, \"RDE\", \"NRS_rev\", \"p99\", \"Worst1perc\"]\n",
    "\n",
    "competitors_to_use = [\"Naive-Baseline\", \"LAET\", \"HNSW\", \"DARTH\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 40})\n",
    "bar_width = 0.2\n",
    "\n",
    "for measure in all_measures:\n",
    "    measure_label = quality_measures[measure][\"label\"]\n",
    "    measure_calculator = quality_measures[measure][\"calculator\"]\n",
    "    \n",
    "    average_measure_improvement_DARTH_vs_LAET = 0\n",
    "    average_measure_improvement_DARTH_vs_HNSW = 0\n",
    "    average_measure_improvement_DARTH_vs_baseline = 0\n",
    "    average_measure_calc_num = 0\n",
    "    \n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "        naive_values, DARTH_values, LAET_values, HNSW_values = [], [], [], []\n",
    "        naive_errors, DARTH_errors, LAET_errors, HNSW_errors = [], [], [], []  # For storing standard errors\n",
    "        \n",
    "        if ds_name == \"T2I100M\":\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\"]\n",
    "        else:\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "        \n",
    "        for r_target in all_r_targets:\n",
    "            DARTH_values_k = []\n",
    "            LAET_values_k = []\n",
    "            naive_values_k = []\n",
    "            HNSW_values_k = []\n",
    "            \n",
    "            for k in all_k_values:\n",
    "                ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "                f_r_target = float(r_target)\n",
    "                \n",
    "                no_early_stop_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]                \n",
    "                \n",
    "                darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                naive_df = pd.read_csv(get_naive_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                laet_df = pd.read_csv(get_laet_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                hnsw_df = pd.read_csv(get_classic_hnsw_early_stop_testing_dataset_name(M, efC, classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"], s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                \n",
    "                darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                naive_df = naive_df[naive_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                laet_df = laet_df[laet_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                \n",
    "                measure_darth = measure_calculator(darth_df, s, f_r_target, recall_col=\"r_actual\")\n",
    "                measure_laet = measure_calculator(laet_df, s, f_r_target)\n",
    "                measure_naive = measure_calculator(naive_df, s, f_r_target)\n",
    "                measure_hnsw = measure_calculator(hnsw_df, s, f_r_target)\n",
    "                \n",
    "                DARTH_values_k.append(measure_darth)\n",
    "                LAET_values_k.append(measure_laet)\n",
    "                naive_values_k.append(measure_naive)\n",
    "                HNSW_values_k.append(measure_hnsw)\n",
    "                \n",
    "                # Calculate relative improvement\n",
    "                average_measure_improvement_DARTH_vs_LAET += measure_darth / measure_laet\n",
    "                average_measure_improvement_DARTH_vs_baseline += measure_darth / measure_naive\n",
    "                average_measure_improvement_DARTH_vs_HNSW += measure_darth / measure_hnsw\n",
    "                average_measure_calc_num += 1\n",
    "\n",
    "            naive_values.append(np.mean(naive_values_k))\n",
    "            DARTH_values.append(np.mean(DARTH_values_k))\n",
    "            LAET_values.append(np.mean(LAET_values_k))\n",
    "            HNSW_values.append(np.mean(HNSW_values_k))\n",
    "\n",
    "            # Calculate standard error for each set of values\n",
    "            naive_errors.append(np.std(naive_values_k) / np.sqrt(len(naive_values_k)))\n",
    "            DARTH_errors.append(np.std(DARTH_values_k) / np.sqrt(len(DARTH_values_k)))\n",
    "            LAET_errors.append(np.std(LAET_values_k) / np.sqrt(len(LAET_values_k)))\n",
    "            HNSW_errors.append(np.std(HNSW_values_k) / np.sqrt(len(HNSW_values_k)))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "        x = np.arange(len(all_r_targets))        \n",
    "        competitor_dict = {\n",
    "            \"HNSW\": HNSW_values,\n",
    "            \"LAET\": LAET_values,\n",
    "            \"Naive-Baseline\": naive_values,\n",
    "            \"DARTH\": DARTH_values\n",
    "        }\n",
    "        \n",
    "        if quality_measures[measure][\"type\"] == \"bar\":\n",
    "            num_competitors = len(competitors_to_use)\n",
    "            offset = (0 - num_competitors / 2) * bar_width + bar_width / 2\n",
    "            for j, competitor in enumerate(competitors_to_use):\n",
    "                ax.bar(x + offset + j * bar_width, competitor_dict[competitor], width=bar_width, label=competitor, color=algorithm_conf[competitor][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "            ax.grid(alpha=0.8, axis=\"y\", linestyle=\"--\")\n",
    "        else:\n",
    "            num_competitors = len(competitors_to_use)\n",
    "            offset = (0 - num_competitors / 2) * bar_width + bar_width / 2\n",
    "            for j, competitor in enumerate(competitors_to_use):\n",
    "                ax.plot(x, competitor_dict[competitor], color=algorithm_conf[competitor][\"color\"], marker=algorithm_conf[competitor][\"marker\"], label=algorithm_conf[competitor][\"label\"], markersize=15)\n",
    "            ax.grid(alpha=0.8, linestyle=\"--\")\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        \n",
    "        ax.set_xticks(np.arange(len(all_r_targets)))\n",
    "        ax.set_xticklabels(all_r_targets)\n",
    "        ax.set_xlabel(r\"$R_t$\")\n",
    "        ax.set_ylabel(f\"{measure_label}\")\n",
    "        #ax.tick_params(axis='x', labelsize=40)\n",
    "        ax.set_xticklabels([\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"])\n",
    "\n",
    "        fig.tight_layout()\n",
    "        #ax.legend(fontsize=20, loc=\"upper right\", borderpad=0.3, ncol=2)\n",
    "        \n",
    "        if len(all_k_values) == 1:\n",
    "            savefile = f\"{PLOTS_DIR}comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\"\n",
    "        else:\n",
    "            savefile = f\"{PLOTS_DIR}comparisons_{measure}_{ds_name}.pdf\"\n",
    "        fig.savefig(savefile, bbox_inches=\"tight\")\n",
    "\n",
    "        print(f\"Saved plot at {savefile}\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    \n",
    "    average_measure_improvement_DARTH_vs_LAET = average_measure_improvement_DARTH_vs_LAET / average_measure_calc_num\n",
    "    average_measure_improvement_DARTH_vs_HNSW = average_measure_improvement_DARTH_vs_HNSW / average_measure_calc_num    \n",
    "    average_measure_improvement_DARTH_vs_baseline = average_measure_improvement_DARTH_vs_baseline / average_measure_calc_num\n",
    "    print(f\"{measure} improvement of DARTH vs LAET: {average_measure_improvement_DARTH_vs_LAET:.2f}\")\n",
    "    print(f\"{measure} improvement of DARTH vs HNSW: {average_measure_improvement_DARTH_vs_HNSW:.2f}\")\n",
    "    print(f\"{measure} improvement of DARTH vs Baseline: {average_measure_improvement_DARTH_vs_baseline:.2f}\")\n",
    "    \n",
    "    all_average_improvements = [average_measure_improvement_DARTH_vs_LAET, average_measure_improvement_DARTH_vs_HNSW, average_measure_improvement_DARTH_vs_baseline]\n",
    "    print(f\"Median improvement overall: {np.median(all_average_improvements)}\")\n",
    "    print(f\"Average improvement overall: {np.mean(all_average_improvements)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QPS graphs but with the actual achieved recall (not the recall target)\n",
    "\n",
    "all_datasets = [\"T2I100M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\"]\n",
    "all_measures = [\"QPS\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 40})\n",
    "bar_width = 0.2\n",
    "\n",
    "for measure in all_measures:\n",
    "    measure_label = quality_measures[measure][\"label\"]\n",
    "    measure_calculator = quality_measures[measure][\"calculator\"]\n",
    "    \n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "        naive_values, DARTH_values, LAET_values, HNSW_values = [], [], [], []\n",
    "        naive_values_achieved_recalls, DARTH_values_achieved_recalls, LAET_values_achieved_recalls, HNSW_values_achieved_recalls = [], [], [], []\n",
    "        \n",
    "        if ds_name == \"T2I100M\":\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\"]\n",
    "        else:\n",
    "            all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "        \n",
    "        for r_target in all_r_targets:\n",
    "            DARTH_values_k = []\n",
    "            LAET_values_k = []\n",
    "            naive_values_k = []\n",
    "            HNSW_values_k = []\n",
    "            \n",
    "            DARTH_values_k_achieved_recalls = []\n",
    "            LAET_values_k_achieved_recalls = []\n",
    "            naive_values_k_achieved_recalls = []\n",
    "            HNSW_values_k_achieved_recalls = []\n",
    "            \n",
    "            for k in all_k_values:\n",
    "                ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "                f_r_target = float(r_target)\n",
    "                \n",
    "                no_early_stop_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]                \n",
    "                \n",
    "                darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                naive_df = pd.read_csv(get_naive_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                laet_df = pd.read_csv(get_laet_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                hnsw_df = pd.read_csv(get_classic_hnsw_early_stop_testing_dataset_name(M, efC, classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"], s, ds_name, k, r_target), usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                \n",
    "                darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                naive_df = naive_df[naive_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                laet_df = laet_df[laet_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                \n",
    "                achieved_recall_darth = darth_df[\"r_actual\"].mean()\n",
    "                achieved_recall_naive = naive_df[\"r\"].mean()\n",
    "                achieved_recall_laet = laet_df[\"r\"].mean()\n",
    "                achieved_recall_hnsw = hnsw_df[\"r\"].mean()\n",
    "                \n",
    "                measure_darth = measure_calculator(darth_df, s, f_r_target, recall_col=\"r_actual\")\n",
    "                measure_laet = measure_calculator(laet_df, s, f_r_target)\n",
    "                measure_naive = measure_calculator(naive_df, s, f_r_target)\n",
    "                measure_hnsw = measure_calculator(hnsw_df, s, f_r_target)\n",
    "                \n",
    "                DARTH_values_k.append(measure_darth)\n",
    "                LAET_values_k.append(measure_laet)\n",
    "                naive_values_k.append(measure_naive)\n",
    "                HNSW_values_k.append(measure_hnsw)\n",
    "                \n",
    "                DARTH_values_k_achieved_recalls.append(achieved_recall_darth)\n",
    "                LAET_values_k_achieved_recalls.append(achieved_recall_laet)\n",
    "                naive_values_k_achieved_recalls.append(achieved_recall_naive)\n",
    "                HNSW_values_k_achieved_recalls.append(achieved_recall_hnsw)\n",
    "                \n",
    "                #print(f\"ds={ds_name} k={k} rt={r_target} meas={measure_label} || DARTH: {measure_darth} LAET: {measure_laet} Naive: {measure_naive}\")\n",
    "            \n",
    "            naive_values.append(np.mean(naive_values_k))\n",
    "            DARTH_values.append(np.mean(DARTH_values_k))\n",
    "            LAET_values.append(np.mean(LAET_values_k))\n",
    "            HNSW_values.append(np.mean(HNSW_values_k))\n",
    "            \n",
    "            naive_values_achieved_recalls.append(np.mean(naive_values_k_achieved_recalls))\n",
    "            DARTH_values_achieved_recalls.append(np.mean(DARTH_values_k_achieved_recalls))\n",
    "            LAET_values_achieved_recalls.append(np.mean(LAET_values_k_achieved_recalls))\n",
    "            HNSW_values_achieved_recalls.append(np.mean(HNSW_values_k_achieved_recalls))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "        x = np.arange(len(all_r_targets))\n",
    "        \n",
    "        \n",
    "        ax.plot(naive_values_achieved_recalls, naive_values, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], marker=algorithm_conf[\"Naive-Baseline\"][\"marker\"], label=algorithm_conf[\"Naive-Baseline\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "        ax.plot(HNSW_values_achieved_recalls, HNSW_values, color=algorithm_conf[\"HNSW\"][\"color\"], marker=algorithm_conf[\"HNSW\"][\"marker\"], label=algorithm_conf[\"HNSW\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "        ax.plot(LAET_values_achieved_recalls, LAET_values, color=algorithm_conf[\"LAET\"][\"color\"], marker=algorithm_conf[\"LAET\"][\"marker\"], label=algorithm_conf[\"LAET\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "        ax.plot(DARTH_values_achieved_recalls, DARTH_values, color=algorithm_conf[\"DARTH\"][\"color\"], marker=algorithm_conf[\"DARTH\"][\"marker\"], label=algorithm_conf[\"DARTH\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "        ax.grid(alpha=0.8, linestyle=\"--\")\n",
    "        \n",
    "        print(\"LAET\", LAET_values)\n",
    "        print(\"DARTH\", DARTH_values)\n",
    "        print(\"HNSW\", HNSW_values)\n",
    "        print(\"naive\", naive_values)\n",
    "        \n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "\n",
    "        ax.set_xticks([0.80, 0.85, 0.90, 0.95, 1])\n",
    "        ax.set_xlabel(\"Actual Recall\")\n",
    "        ax.set_ylabel(f\"{measure_label}\")\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if len(all_k_values) == 1:\n",
    "            filename = f\"../../experiments/plots/comparisons_actual_recall_{measure}_k{all_k_values[0]}_{ds_name}.pdf\"\n",
    "        else:\n",
    "            filename = f\"../../experiments/plots/comparisons_actual_recall_{measure}_{ds_name}.pdf\"\n",
    "        fig.savefig(filename, bbox_inches=\"tight\")\n",
    "\n",
    "        print(f\"Saved plot at {filename}\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localPyLibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
