{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the DARTH approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "from matplotlib.ticker import MaxNLocator  # type: ignore\n",
    "\n",
    "from sklearn.manifold import TSNE  # type: ignore\n",
    "from sklearn.decomposition import PCA  # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.linear_model import LinearRegression  # type: ignore\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "SEED = 42\n",
    "s = 1000\n",
    "\n",
    "def get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi):\n",
    "    return f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\"\n",
    "\n",
    "\n",
    "def get_detailed_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi):\n",
    "    return f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/detailed/M{M}_efC{efC}_efS{efS}_qs{s}_tr{r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\"\n",
    "\n",
    "\n",
    "def get_naive_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, dt):\n",
    "    return f\"../../experiments/results/naive-early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{r_target}_dt{dt}.txt\"\n",
    "\n",
    "\n",
    "def get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k):\n",
    "    return f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\"\n",
    "\n",
    "\n",
    "def get_laet_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target):\n",
    "    return f\"../../experiments/results/laet-early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{r_target:.2f}.txt\"\n",
    "\n",
    "\n",
    "def get_optimal_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k):\n",
    "    return f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/detailed.M{M}_efC{efC}_efS{efS}_qs{s}.txt\"\n",
    "\n",
    "\n",
    "def get_heuristic_interval_conf_filename(version=\"v1\"):\n",
    "    return f\"./heuristic_recommendations_params_{version}.json\"\n",
    "\n",
    "\n",
    "def get_testing_detailed_dataset_name(M, efC, efS, s, ds_name, k, logint): \n",
    "    return f\"../../experiments/results/test_logging/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_li{logint}.txt\"\n",
    "\n",
    "\n",
    "def get_model_name(M, ef, s, ds_name, k, logint, selected_features, n_estimators=100):\n",
    "    return f\"../../predictor_models/lightgbm/{ds_name}_M{M}_ef{ef}_s{s}_k{k}_nestim{n_estimators}_li{logint}_{selected_features}.txt\"\n",
    "\n",
    "def compute_P99(y_true, y_pred):\n",
    "    y_diff = np.abs(y_true - y_pred)\n",
    "    return np.percentile(y_diff, 99)\n",
    "\n",
    "dataset_params = {\n",
    "        \"SIFT100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"lightblue\",\n",
    "            \"marker\": \"o\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"SIFT100M\"\n",
    "        },\n",
    "        \"DEEP100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 750,\n",
    "            \"color\": \"plum\",\n",
    "            \"marker\": \"x\",   \n",
    "            \"li\": 1,\n",
    "            \"label\": \"DEEP100M\"\n",
    "        },\n",
    "        \"T2I100M\": {\n",
    "            \"M\": 80,\n",
    "            \"efC\": 1000,\n",
    "            \"efS\": 2500,\n",
    "            \"color\": \"rosybrown\",\n",
    "            \"marker\": \"d\",\n",
    "            \"li\": 2,\n",
    "            \"label\": \"T2I100M\"\n",
    "        },\n",
    "        \"GLOVE100\": {\n",
    "            \"M\": 16,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"orange\",\n",
    "            \"marker\": \"^\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GLOVE1M\"\n",
    "        },\n",
    "        \"GIST1M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 1000,\n",
    "            \"color\": \"lightgreen\",\n",
    "            \"marker\": \"s\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GIST1M\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Load the dataconf from a file:\n",
    "interval_conf = {}\n",
    "with open(\"../../experiments/generated_json/final_heuristic_adaptive_recommendations_params_ipidiv2_mpidiv10.json\") as f:\n",
    "    interval_conf = json.load(f)\n",
    "\n",
    "print(interval_conf.keys(), interval_conf[\"SIFT100M\"].keys(), interval_conf[\"SIFT100M\"][\"100\"].keys())\n",
    "\n",
    "PLOTS_DIR = \"./../../experiments/revision-plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Prediction Times in DARTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"T2I100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "average_prediction_times = []\n",
    "for ds_name in all_datasets:\n",
    "    M = dataset_params[ds_name][\"M\"]\n",
    "    efC = dataset_params[ds_name][\"efC\"]\n",
    "    efS = dataset_params[ds_name][\"efS\"]\n",
    "    per_k_values = []\n",
    "    for k in all_k_values:\n",
    "        per_recall_values = []\n",
    "        for r_target in all_r_targets:\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            \n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "        \n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, float(r_target), ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            average_per_prediction_times = darth_df[\"r_predictor_time_ms\"] / darth_df[\"r_predictor_calls\"]\n",
    "            \n",
    "            average_per_prediction_time = average_per_prediction_times.mean()\n",
    "            per_recall_values.append(average_per_prediction_time)\n",
    "        per_k_values.append(np.mean(per_recall_values))\n",
    "    \n",
    "    average_prediction_time_ds = np.mean(per_k_values)\n",
    "    #print(f\"Dataset: {ds_name}: {average_prediction_time_ds:.2f} ms\")\n",
    "    \n",
    "    average_prediction_times.append(average_prediction_time_ds)\n",
    "\n",
    "print(f\"Average prediction time: {np.mean(average_prediction_times):.2f} ms\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Test Measures for the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_estimators = 100\n",
    "\n",
    "index_metric_feats = [\"step\", \"dists\", \"inserts\"]\n",
    "neighbor_distances_feats = [\"first_nn_dist\", \"nn_dist\", \"furthest_dist\"]\n",
    "neighbor_stats_feats = [\"avg_dist\", \"variance\", \"percentile_25\", \"percentile_50\", \"percentile_75\"]\n",
    "all_feats = index_metric_feats + neighbor_distances_feats + neighbor_stats_feats\n",
    "\n",
    "columns_to_load = [\"qid\", \"elaps_ms\"] + all_feats + [\"r\", \"feats_collect_time_ms\"]\n",
    "\n",
    "feature_classes = {\n",
    "    \"index_metric_feats\": index_metric_feats,\n",
    "    \"neighbor_distances_feats\": neighbor_distances_feats,\n",
    "    \"neighbor_stats_feats\": neighbor_stats_feats,\n",
    "    \"index_metrics_and_neighbor_distances\": index_metric_feats + neighbor_distances_feats,\n",
    "    \"index_metrics_and_neighbor_stats\": index_metric_feats + neighbor_stats_feats,\n",
    "    \"neighbor_distances_and_neighbor_stats\": neighbor_distances_feats + neighbor_stats_feats,\n",
    "    \"all_feats\": all_feats,\n",
    "}\n",
    "\n",
    "\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    mse_per_k = []\n",
    "    mae_per_k = []\n",
    "    r2_per_k = []\n",
    "    p99_per_k = []\n",
    "    max_error_per_k = []\n",
    "    \n",
    "    M = dataset_params[ds_name][\"M\"]\n",
    "    efC = dataset_params[ds_name][\"efC\"]\n",
    "    efS = dataset_params[ds_name][\"efS\"]\n",
    "    \n",
    "    for k in all_k_values:\n",
    "        \n",
    "        testing_data_df = pd.read_csv(get_testing_detailed_dataset_name(M, efC, efS, 1000, ds_name, k, 1), usecols=columns_to_load)\n",
    "        testing_y_true = testing_data_df[\"r\"]\n",
    "                    \n",
    "        feats = all_feats\n",
    "        li = dataset_params[ds_name][\"li\"]\n",
    "        model_file = f\"../../predictor_models/darth/{ds_name}_M{M}_efC{efC}_efS{efS}_s{10000}_k{k}_nestim{n_estimators}_li{li}_all_feats.txt\"\n",
    "        model = lgb.Booster(model_file=model_file)\n",
    "                    \n",
    "        validation_X = testing_data_df[feats]\n",
    "        validation_y_pred = model.predict(validation_X)\n",
    "        mse = mean_squared_error(testing_y_true, validation_y_pred)\n",
    "        mae = mean_absolute_error(testing_y_true, validation_y_pred)\n",
    "        r2 = r2_score(testing_y_true, validation_y_pred)\n",
    "        p99 = compute_P99(testing_y_true, validation_y_pred)\n",
    "        max_error = np.max(np.abs(testing_y_true - validation_y_pred))\n",
    "        \n",
    "        mse_per_k.append(mse)\n",
    "        mae_per_k.append(mae)\n",
    "        r2_per_k.append(r2)\n",
    "        p99_per_k.append(p99)\n",
    "        max_error_per_k.append(max_error)\n",
    "        \n",
    "        #print(f\"mse: {mse:.4f}, mae: {mae:.4f}, r2: {r2:.4f}, p99: {p99:.4f}\")\n",
    "        print(f\"    {ds_name} k={k} => mse: {mse:.4f}, mae: {mae:.4f}, r2: {r2:.2f}, p99: {p99:.4f}, max_error: {max_error:.4f}\")\n",
    "\n",
    "    mse_ds = np.mean(mse_per_k)\n",
    "    mae_ds = np.mean(mae_per_k)\n",
    "    r2_ds = np.mean(r2_per_k)\n",
    "    p99_ds = np.mean(p99_per_k)\n",
    "    max_error_ds = np.mean(max_error_per_k)\n",
    "    \n",
    "    print(f\"{ds_name} => mse: {mse_ds:.4f}, mae: {mae_ds:.4f}, r2: {r2_ds:.2f}, p99: {p99_ds:.4f}, max_error: {max_error_ds:.4f}\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of performance of DARTH approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate all speedups and recall performance for all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for all datasets and all k-s the average recall vs the target recall\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "all_speedups = [] #meant to extract analytics\n",
    "\n",
    "ds_recalls = {} # k -> dataset -> [recall, std, qut, speedup for each r_target]\n",
    "\n",
    "for k in all_k_values:\n",
    "    ds_recalls[k] = {}\n",
    "    print(f\"k: {k}\")\n",
    "    for ds_name in all_datasets:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        print(f\"    Dataset: {ds_name}\")\n",
    "        ds_recalls[k][ds_name] = []\n",
    "        \n",
    "        for r_target in all_r_targets:\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                ds_recalls[k][ds_name].append([])\n",
    "                continue\n",
    "            \n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "            r_target = float(r_target)\n",
    "                \n",
    "            #print(f\"r_target: {r_target}, ipi: {ipi}, mpi: {mpi}\")\n",
    "                \n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            \n",
    "            # Important:\n",
    "            # For the given recall target, find which queries do not reach the target recall in general (from the no-early-termination runs)\n",
    "            # Then, remove those queries from both the no-early-termination and the DARTH runs\n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target]\n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_termination_df[\"qid\"])]\n",
    "            \n",
    "            darth_recall_avg = darth_df[\"r_actual\"].mean()\n",
    "            darth_recall_std = darth_df[\"r_actual\"].std()\n",
    "            darth_df[\"r_actual\"] = darth_df[\"r_actual\"].apply(lambda x: round(x, 2))\n",
    "            darth_rqut = len(darth_df[darth_df[\"r_actual\"] < r_target])\n",
    "            \n",
    "            speedup = no_early_termination_df[\"elaps_ms\"].mean() / darth_df[\"elaps_ms\"].mean()\n",
    "            \n",
    "            per_query_speedups = no_early_termination_df[\"elaps_ms\"] / darth_df[\"elaps_ms\"]\n",
    "            speedup_v2 = per_query_speedups.mean()\n",
    "            \n",
    "            all_speedups.append(speedup_v2)\n",
    "            \n",
    "            ds_recalls[k][ds_name].append([darth_recall_avg, darth_recall_std, darth_rqut, speedup, speedup_v2])\n",
    "            \n",
    "            print(f\"        r_target: {r_target}, darth_recall_avg: {darth_recall_avg:.4f}, darth_recall_std: {darth_recall_std:.4f}, darth_rqut: {darth_rqut}, speedup_v2: {speedup_v2:.2f}x\")            \n",
    "\n",
    "#print(f\"Overall average speedup: {overall_average_speedup / overall_average_speedup_times:.2f}x\")\n",
    "print(f\"Overall average speedup: {np.mean(all_speedups):.2f}x\")\n",
    "print(f\"Overall median speedup: {np.median(all_speedups):.2f}x\")\n",
    "print(f\"Max speedup: {np.max(all_speedups):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the plots for all normal datasets (except T2I100M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 40})\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))\n",
    "legend_elements = [Patch(facecolor=dataset_params[ds_name][\"color\"], edgecolor=dataset_params[ds_name][\"color\"], alpha=0.8, label=ds_name) for ds_name in [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False, handletextpad=0.3,  columnspacing=0.5, labelspacing=0.3 )\n",
    "ax_legend.axis('off')\n",
    "fig_legend.savefig(f\"{PLOTS_DIR}summary_speedups_legend_only.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close(fig_legend)\n",
    "\n",
    "for k in all_k_values:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.2\n",
    "    x = np.arange(len(all_r_targets))\n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        recalls = []\n",
    "        x_positions = x + i * bar_width\n",
    "\n",
    "        for j, r_target in enumerate(all_r_targets):\n",
    "            recall = ds_recalls[k][ds_name][j][0]\n",
    "            recall_std = ds_recalls[k][ds_name][j][1]\n",
    "            recalls.append(recall)\n",
    "            std = recall_std\n",
    "            ax.bar(x_positions[j], recall, bar_width, color=dataset_params[ds_name][\"color\"], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "            \n",
    "    ax.set_xticks(x + (len(all_datasets) - 1) * bar_width / 2)\n",
    "    ax.set_yticks([float(r) for r in [0.80, 0.85, 0.90, 0.95, 0.99]])\n",
    "    ax.set_xlabel(r\"$R_t$\")\n",
    "    ax.set_ylabel(\"Actual Recall\")\n",
    "    ax.set_xticklabels([str(t) for t in all_r_targets])\n",
    "    ax.grid(alpha=0.8, axis='y', linestyle='--')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.set_ylim(0.78, 1.00)\n",
    "    #ax.tick_params(axis='x', labelsize=38)\n",
    "    fig.tight_layout()\n",
    "    savepath = f\"{PLOTS_DIR}summary_recall_vs_target_recall_k{k}.pdf\"\n",
    "    fig.savefig(savepath, bbox_inches='tight')\n",
    "    print(f\"Fig saved at {savepath}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # SPEEDUP PLOT\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bar_width = 0.2\n",
    "    x = np.arange(len(all_r_targets))\n",
    "    max_speedup = -1\n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        x_positions = x + i * bar_width\n",
    "        speedup_version_to_use=4\n",
    "        speedups = []\n",
    "        for j, r_target in enumerate(all_r_targets):\n",
    "            speedup = ds_recalls[k][ds_name][j][speedup_version_to_use]\n",
    "            if speedup > max_speedup:\n",
    "                max_speedup = speedup\n",
    "            speedups.append(speedup)\n",
    "            ax.bar(x_positions[j], speedup, bar_width, color=dataset_params[ds_name][\"color\"], alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    \n",
    "    ax.set_xticks(x + (len(all_datasets) - 1) * bar_width / 2)\n",
    "    ax.set_xlabel(r\"$R_t$\")\n",
    "    ax.set_ylabel(\"Times Faster\")\n",
    "    ax.set_yticks(np.arange(1, max_speedup + 1, 5))\n",
    "    ax.set_ylim(bottom=1)\n",
    "    ax.set_xticklabels([str(t) for t in all_r_targets])\n",
    "    ax.grid(alpha=0.8, axis='y', linestyle='--')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.set_yticks([1, 4, 8, 12, 16])\n",
    "\n",
    "    #ax.tick_params(axis='x', labelsize=38)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    filename = f\"{PLOTS_DIR}summary_speedup_vs_target_recall_k{k}.pdf\"\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "    print(f\"Fig saved at {filename}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the plots for the T2I100M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for all datasets and all k-s the average recall vs the target recall\n",
    "all_datasets = [\"T2I100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\"]\n",
    "bar_width = 0.17\n",
    "alpha = 0.8\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "k_colors = {\n",
    "    \"10\": \"#1f77b4\",  # Blue\n",
    "    \"25\": \"#ff7f0e\",  # Orange\n",
    "    \"50\": \"#2ca02c\",  # Green\n",
    "    \"75\": \"#d62728\",  # Red\n",
    "    \"100\": \"#9467bd\"  # Purple\n",
    "}\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))\n",
    "legend_elements = [Patch(facecolor=k_colors[k], edgecolor=k_colors[k], alpha=0.8, label=f\"k={k}\") for k in [\"10\", \"25\", \"50\", \"75\", \"100\"]]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=5, frameon=False, handletextpad=0.3,  columnspacing=0.5, labelspacing=0.3 )\n",
    "ax_legend.axis('off')\n",
    "filename = f\"{PLOTS_DIR}summary_speedups_legend_only_k_bars.pdf\"\n",
    "fig_legend.savefig(filename, bbox_inches=\"tight\")\n",
    "print(f\"Fig saved at {filename}\")\n",
    "plt.show()\n",
    "plt.close(fig_legend)\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = np.arange(len(all_r_targets))\n",
    "    \n",
    "    for i, k in enumerate(all_k_values):\n",
    "        x_positions = x + i * bar_width\n",
    "        recalls = []\n",
    "        for j, r_target in enumerate(all_r_targets):\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            recall = ds_recalls[k][ds_name][j][0]\n",
    "            recall_std = ds_recalls[k][ds_name][j][1]\n",
    "            recalls.append(recall)\n",
    "            ax.bar(x_positions[j], recall, bar_width, \n",
    "                   label=f\"k={k}\" if j == 0 and k in {\"10\", \"25\"} else \"\", \n",
    "                   color=k_colors[k], alpha=alpha, edgecolor='black', linewidth=1)\n",
    "        \n",
    "    ax.set_xticks(x + (len(all_k_values) - 1) * bar_width / 2)\n",
    "    ax.set_yticks([0.80, 0.85, 0.90, 0.95, 0.99])\n",
    "    ax.set_xlabel(r\"$R_t$\")\n",
    "    ax.set_ylabel(\"Actual Recall\")\n",
    "    ax.set_xticklabels([str(t) for t in all_r_targets])\n",
    "    #ax.tick_params(axis='x', labelsize=38)\n",
    "    ax.grid(alpha=0.8, axis='y', linestyle='--')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.set_ylim(0.78, 1.00)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    filename = f\"{PLOTS_DIR}summary_recall_vs_target_recall_ds{ds_name}.pdf\"\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "    print(f\"Fig saved at {filename}\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    max_speedup = -1\n",
    "    for i, k in enumerate(all_k_values):\n",
    "        x_positions = x + i * bar_width\n",
    "        speedups = []\n",
    "        for j, r_target in enumerate(all_r_targets):\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            speedup = ds_recalls[k][ds_name][j][4]\n",
    "            max_speedup = max(max_speedup, speedup)\n",
    "            speedups.append(speedup)\n",
    "            ax.bar(x_positions[j], speedup, bar_width, \n",
    "                   label=f\"k={k}\" if j == 0 and k in {\"50\", \"75\", \"100\"} else \"\", \n",
    "                   color=k_colors[k], alpha=alpha, edgecolor='black', linewidth=1)\n",
    "        \n",
    "    ax.set_xticks(x + (len(all_k_values) - 1) * bar_width / 2)\n",
    "    ax.set_xlabel(r\"$R_t$\")\n",
    "    ax.set_ylabel(\"Times Faster\")\n",
    "    ax.set_yticks(np.arange(1, max_speedup + 1, 5))\n",
    "    ax.set_ylim(bottom=1)\n",
    "    ax.set_xticklabels([str(t) for t in all_r_targets])\n",
    "    ax.grid(alpha=0.8, axis='y', linestyle='--')\n",
    "    #ax.tick_params(axis='x', labelsize=38)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    filename = f\"{PLOTS_DIR}summary_speedup_vs_target_recall_ds{ds_name}.pdf\"\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(filename, bbox_inches='tight')\n",
    "    print(f\"Fig saved at {filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results of our Early Termination Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 42}) \n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))  # Adjust the figsize for better alignment\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"dimgray\", edgecolor=\"dimgray\", alpha=0.8, label=\"plain HNSW\"),\n",
    "    Patch(facecolor=\"tomato\", edgecolor=\"tomato\", alpha=0.8, label=\"DARTH\"),\n",
    "    #Line2D([0], [0], color=\"tomato\", linestyle=\"dashed\", linewidth=5, label=\"DARTH Avg.\"),\n",
    "    #Line2D([0], [0], color=\"dimgray\", linestyle=\"dashed\", linewidth=5, label=\"HNSW No Early Termination Avg.\"),\n",
    "    #Line2D([0], [0], color=\"black\", linestyle=\"dashed\", linewidth=5, label=r\"$R_t$\"), \n",
    "]\n",
    "ax_legend.legend(handles=legend_elements, loc='center', ncol=5, frameon=False)\n",
    "\n",
    "ax_legend.axis('off')\n",
    "fig_legend.savefig(f\"{PLOTS_DIR}perf_legend_only.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close(fig_legend)\n",
    "\n",
    "s = 1000\n",
    "all_datasets = [\"SIFT100M\"]\n",
    "all_k_values = [\"50\"]\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        r_targets = interval_conf[ds_name][k].keys()\n",
    "\n",
    "        if ds_name == \"T2I100M\":\n",
    "            r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\"]\n",
    "        else:\n",
    "            r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "        for ir, r_target in enumerate(r_targets):\n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "            r_target = float(r_target)\n",
    "                        \n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "            baseline_mean = no_early_termination_df[\"elaps_ms\"].mean()\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            elapsed_mean = darth_df[\"elaps_ms\"].mean()\n",
    "            predictor_calls_mean = darth_df[\"r_predictor_calls\"].mean()\n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target]\n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_termination_df[\"qid\"])]\n",
    "            \n",
    "            fig_recall, ax_recall = plt.subplots(figsize=(10, 6))\n",
    "            ax_recall.hist(darth_df[\"r_actual\"], alpha=0.8, color=\"tomato\", linewidth=2.0, label=\"DARTH\")\n",
    "            r_actual_mean = darth_df[\"r_actual\"].mean()\n",
    "            ax_recall.axvline(r_actual_mean, color=\"tomato\", linestyle=\"dashed\", linewidth=4, label=f\"avg:{r_actual_mean:.2f}\")\n",
    "            ax_recall.axvline(r_target, color=\"black\", linestyle=\"dashed\", linewidth=4, label=r\"$R_t$\")\n",
    "            for spine in ax_recall.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            ax_recall.set_ylabel(\"# Queries\")\n",
    "            ax_recall.set_xlabel(f\"Actual Recall\")\n",
    "            ax_recall.grid(alpha=0.8, linestyle=\"--\")\n",
    "            ax_recall.legend(fontsize=38, handletextpad=0.0, columnspacing=0.0, labelspacing=0.0, frameon=False, handlelength=0.8)\n",
    "            fig_recall.tight_layout()\n",
    "            fig_recall.savefig(f\"{PLOTS_DIR}perf_recall_{ds_name}_k{k}_{r_target}.pdf\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "            plt.close(fig_recall)\n",
    "            \n",
    "            #fig_predictor_calls, ax_predictor_calls = plt.subplots(figsize=(10, 6))\n",
    "            #ax_predictor_calls.hist(darth_df[\"r_predictor_calls\"], alpha=0.8, color=\"tomato\", edgecolor=\"black\", linewidth=2.0)\n",
    "            #ax_predictor_calls.axvline(predictor_calls_mean, color=\"tomato\", linestyle=\"dashed\", linewidth=4, label=f\"DARTH avg:{round(predictor_calls_mean)}\")\n",
    "            #for spine in ax_predictor_calls.spines.values():\n",
    "            #    spine.set_visible(False)\n",
    "            #ax_predictor_calls.set_ylabel(\"# Queries\")\n",
    "            #ax_predictor_calls.set_xlabel(\"Predictor Calls\")\n",
    "            #ax_predictor_calls.grid(alpha=0.8, linestyle=\"--\")\n",
    "            #ax_predictor_calls.legend(fontsize=24)\n",
    "            #fig_predictor_calls.savefig(f\"{PLOTS_DIR}perf_predictor_calls_{ds_name}_k{k}_{r_target}.pdf\", bbox_inches=\"tight\")\n",
    "            #plt.show()\n",
    "            #plt.close(fig_predictor_calls)\n",
    "\n",
    "            fig_elapsed, ax_elapsed = plt.subplots(figsize=(10, 6))\n",
    "            ax_elapsed.hist(darth_df[\"elaps_ms\"], alpha=0.8, color=\"tomato\", linewidth=2.0, label=\"DARTH\")\n",
    "            ax_elapsed.hist(no_early_termination_df[\"elaps_ms\"], alpha=0.8, color=\"dimgray\", linewidth=2.0, label=\"plain HNSW\")\n",
    "            elapsed_mean = darth_df[\"elaps_ms\"].mean()\n",
    "            print(\"Speedup for DARTH: \", baseline_mean / elapsed_mean)\n",
    "            ax_elapsed.axvline(elapsed_mean, color=\"tomato\", linestyle=\"dashed\", linewidth=4, label=f\"avg:{elapsed_mean:.0f}ms\")\n",
    "            ax_elapsed.axvline(baseline_mean, color=\"dimgray\", linestyle=\"dashed\", linewidth=4, label=f\"avg:{baseline_mean:.0f}ms\")\n",
    "            for spine in ax_elapsed.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            ax_elapsed.set_ylabel(\"# Queries\")\n",
    "            ax_elapsed.set_xlabel(\"Search Time (ms)\")\n",
    "            ax_elapsed.grid(alpha=0.8, linestyle=\"--\")\n",
    "            ax_elapsed.legend(fontsize=34, handletextpad=0.0, columnspacing=0.0, labelspacing=0.0, frameon=False, handlelength=0.8, loc=\"upper right\")\n",
    "            fig_elapsed.tight_layout()\n",
    "            fig_elapsed.savefig(f\"{PLOTS_DIR}perf_search_time_{ds_name}_k{k}_{r_target}.pdf\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "            plt.close(fig_elapsed)\n",
    "            \n",
    "            #fig_dists, ax_dists = plt.subplots(figsize=(8, 4))\n",
    "            #ax_dists.hist(darth_df[\"dists\"], alpha=0.7, color=\"tomato\", edgecolor=\"black\", linewidth=2.0)\n",
    "            #ax_dists.hist(no_early_termination_df[\"dists\"], alpha=0.7, color=\"dimgray\", edgecolor=\"black\", linewidth=2.0)\n",
    "            #dists_mean = darth_df[\"dists\"].mean()\n",
    "            #baseline_dists_mean = no_early_termination_df[\"dists\"].mean()\n",
    "            #ax_dists.axvline(dists_mean, color=\"tomato\", linestyle=\"dashed\", linewidth=4, label=f\"{dists_mean:.0f}\")\n",
    "            #ax_dists.axvline(baseline_dists_mean, color=\"dimgray\", linestyle=\"dashed\", linewidth=4, label=f\"{baseline_dists_mean:.0f}\")\n",
    "            #for spine in ax_dists.spines.values():\n",
    "            #    spine.set_visible(False)\n",
    "            #ax_dists.set_ylabel(\"# Queries\")\n",
    "            #ax_dists.set_xlabel(\"Dists\")\n",
    "            #ax_dists.grid(alpha=0.7, linestyle=\"--\")\n",
    "            #ax_dists.legend()\n",
    "            #fig_dists.savefig(f\"./../../experiments/plots/perf_dists_{ds_name}_k{k}_{r_target}.pdf\", bbox_inches=\"tight\")\n",
    "            #print(f\"Fig saved at ./../../experiments/plots/perf_dists_{ds_name}_k{k}_{r_target}.pdf\")\n",
    "            #plt.show()\n",
    "            #plt.close(fig_dists)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Probably to remove..] Rev. Proposal: Distribution of Distances for plain HNSW:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 40}) \n",
    "\n",
    "s = 1000\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]#, \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "\n",
    "        \n",
    "        no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "        no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target]\n",
    "            \n",
    "        fig_dists, ax_dists = plt.subplots(figsize=(8, 4))\n",
    "        ax_dists.hist(no_early_termination_df[\"dists\"], alpha=0.7, color=\"dimgray\", edgecolor=\"black\", linewidth=2.0)\n",
    "        for spine in ax_dists.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        ax_dists.set_ylabel(\"# Queries\")\n",
    "        ax_dists.set_xlabel(\"Dists\")\n",
    "        ax_dists.grid(alpha=0.7, linestyle=\"--\")\n",
    "        fig_dists.savefig(f\"./../../experiments/plots/plain_HNSW_dists_{ds_name}_k{k}.pdf\", bbox_inches=\"tight\")\n",
    "        print(f\"Fig saved at ./../../experiments/plots/plain_HNSW_dists_{ds_name}_k{k}.pdf\")\n",
    "        plt.show()\n",
    "        plt.close(fig_dists)\n",
    "        \n",
    "        # Do the same for their recall\n",
    "        #fig_recall, ax_recall = plt.subplots(figsize=(8, 4))\n",
    "        #ax_recall.hist(no_early_termination_df[\"r\"], alpha=0.7, color=\"dimgray\", edgecolor=\"black\", linewidth=2.0)\n",
    "        #for spine in ax_recall.spines.values():\n",
    "        #    spine.set_visible(False)\n",
    "        #ax_recall.set_ylabel(\"# Queries\")\n",
    "        #ax_recall.set_xlabel(\"Recall\")\n",
    "        #ax_recall.grid(alpha=0.7, linestyle=\"--\")\n",
    "        #fig_recall.savefig(f\"./../../experiments/plots/plain_HNSW_recall_{ds_name}_k{k}.pdf\", bbox_inches=\"tight\")\n",
    "        #print(f\"Fig saved at ./../../experiments/plots/plain_HNSW_recall_{ds_name}_k{k}.pdf\")\n",
    "        #plt.show()\n",
    "        #plt.close(fig_recall)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the Errors of the DARTH approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\"] #\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\", \"T2I100M\"]\n",
    "all_k_values = [\"100\", \"75\", \"50\", \"25\", \"10\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "# For the queries we surpassed the target recall, find how many distance calculations we did more than needed\n",
    "# For the queries we did not reach the target recall, find how many distance calculations we needed to do more.\n",
    "\n",
    "ds_recalls = {} # k -> dataset -> [recall, std, qut, speedup for each r_target]\n",
    "\n",
    "for k in all_k_values:\n",
    "    ds_recalls[k] = {}\n",
    "    print(f\"k: {k}\")\n",
    "    for ds_name in all_datasets:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        print(f\"    Dataset: {ds_name}\")\n",
    "        ds_recalls[k][ds_name] = []\n",
    "        \n",
    "        for r_target in all_r_targets:\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            \n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "            r_target_fl = float(r_target)\n",
    "                \n",
    "            #print(f\"r_target: {r_target}, ipi: {ipi}, mpi: {mpi}\")\n",
    "                \n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target_fl, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            no_early_termination_detailed_df = pd.read_csv(get_optimal_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "                    \n",
    "            # for each one of the target recalls, find the minimum dist calcs that we needed to do to reach that recall, and put it in a column named min_dists_XX where XX is the recall\n",
    "            grouped_detailed_no_early_termination = no_early_termination_detailed_df.groupby(\"qid\")\n",
    "            for qid, group in grouped_detailed_no_early_termination:\n",
    "                min_dists = group[group[\"r\"] >= r_target_fl][\"dists\"].min()\n",
    "                min_time = group[group[\"r\"] >= r_target_fl][\"elaps_ms\"].min()\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_dists_{r_target}\"] = min_dists\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_time_{r_target}\"] = min_time\n",
    "                #darth_df.loc[darth_df[\"qid\"] == qid, f\"min_dists_{r_target}\"] = min_dists\n",
    "            \n",
    "            # Important:\n",
    "            # For the given recall target, find which queries do not reach the target recall in general (from the no-early-termination runs)\n",
    "            # Then, remove those queries from both the no-early-termination and the DARTH runs\n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target_fl]\n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_termination_df[\"qid\"])]\n",
    "            \n",
    "            # Merge darth and no_early_termination_df\n",
    "            rename_dict = {\n",
    "                \"elaps_ms\": \"total_elaps_ms\",\n",
    "                \"dists\": \"total_dists\",\n",
    "                \"inserts\": \"total_inserts\",\n",
    "                \"r\": \"total_r\",\n",
    "                \"step\": \"total_step\",\n",
    "            }\n",
    "            no_early_termination_df = no_early_termination_df.rename(columns=rename_dict)\n",
    "            darth_df = darth_df.merge(no_early_termination_df, on=\"qid\", how=\"inner\")\n",
    "            \n",
    "            darth_queries_above_target = darth_df[darth_df[\"r_actual\"] >= r_target_fl].copy()\n",
    "            darth_queries_below_target = darth_df[darth_df[\"r_actual\"] < r_target_fl].copy()\n",
    "            \n",
    "            # For the queries we surpassed the target recall, find how many distance calculations we did more than needed\n",
    "            darth_queries_above_target[\"dists_diff\"] = darth_queries_above_target[\"dists\"] - darth_queries_above_target[f\"min_dists_{r_target}\"]\n",
    "            \n",
    "            # For the queries we did not reach the target recall, find how many distance calculations we needed to do more.\n",
    "            darth_queries_below_target[\"dists_diff\"] = darth_queries_below_target[f\"min_dists_{r_target}\"] - darth_queries_below_target[\"dists\"]\n",
    "            \n",
    "            more_dists_than_needed_avg = darth_queries_above_target[\"dists_diff\"].mean()\n",
    "            less_dist_than_needed_avg = darth_queries_below_target[\"dists_diff\"].mean()\n",
    "            avg_distance_calculations = darth_df[\"total_dists\"].mean()\n",
    "            \n",
    "            perc_more_dists_than_needed = 100*more_dists_than_needed_avg / avg_distance_calculations\n",
    "            perc_less_dist_than_needed = 100*less_dist_than_needed_avg / avg_distance_calculations       \n",
    "            \n",
    "            print(f\"        r_target: {r_target} - More dists than needed: {more_dists_than_needed_avg:.0f} ({perc_more_dists_than_needed:.0f}%), Less dists than needed: {less_dist_than_needed_avg:.0f} ({perc_less_dist_than_needed:.0f}%), Avg dist calcs: {avg_distance_calculations:.0f}\")\n",
    "        print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Calculation Optimality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\"font.size\": 48})\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=\"black\", marker=\"^\", linestyle=\"dashed\", linewidth=4, label=\"Total\", markersize=30),\n",
    "    Line2D([0], [0], color=\"gray\", marker=\"s\",linestyle=\"dashed\", linewidth=4, label=\"Optimal\", markersize=30), \n",
    "    Line2D([0], [0], color=\"tomato\", marker=\"o\", linewidth=4, label=\"DARTH\", markersize=30),\n",
    "]\n",
    "ax_legend.legend(\n",
    "    handles=legend_elements, \n",
    "    loc='center', \n",
    "    ncol=3, \n",
    "    frameon=False, \n",
    "    handletextpad=0.3,  # Adjust padding between handle and text\n",
    "    columnspacing=0.5,  # Adjust space between columns\n",
    "    labelspacing=0.3    # Adjust vertical space between labels\n",
    ")\n",
    "ax_legend.axis('off')\n",
    "filename = f\"{PLOTS_DIR}optimality_legends_only.pdf\"\n",
    "fig_legend.savefig(filename, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close(fig_legend)\n",
    "\n",
    "all_datasets = [\"T2I100M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "# For the queries we surpassed the target recall, find how many distance calculations we did more than needed\n",
    "# For the queries we did not reach the target recall, find how many distance calculations we needed to do more.\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "\n",
    "average_dist_diffs = []\n",
    "for k in all_k_values:\n",
    "    print(f\"k: {k}\")\n",
    "    \n",
    "    average_dist_diffs_k = []\n",
    "    for ds_name in all_datasets:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        \n",
    "        optimal_dists_per_r = []\n",
    "        dists_by_DARTH_per_r = []\n",
    "        total_dists_per_r = []\n",
    "        \n",
    "        if ds_name == \"T2I100M\":\n",
    "            r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\"]\n",
    "        else:\n",
    "            r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "        \n",
    "        average_dist_diffs_k_r_target = []\n",
    "        for r_target in r_targets:\n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "            r_target_fl = float(r_target)\n",
    "                                \n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target_fl, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            no_early_termination_detailed_df = pd.read_csv(get_optimal_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "                    \n",
    "            # for each one of the target recalls, find the minimum dist calcs that we needed to do to reach that recall, and put it in a column named min_dists_XX where XX is the recall\n",
    "            grouped_detailed_no_early_termination = no_early_termination_detailed_df.groupby(\"qid\")\n",
    "            for qid, group in grouped_detailed_no_early_termination:\n",
    "                min_dists = group[group[\"r\"] >= r_target_fl][\"dists\"].min()\n",
    "                min_time = group[group[\"r\"] >= r_target_fl][\"elaps_ms\"].min()\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_dists_{r_target}\"] = min_dists\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_time_{r_target}\"] = min_time\n",
    "            \n",
    "            total_dists_r_target = no_early_termination_df[\"dists\"].mean()\n",
    "            \n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target_fl]\n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_termination_df[\"qid\"])]\n",
    "        \n",
    "            optimal_dists_r_target = no_early_termination_df[f\"min_dists_{r_target}\"].mean()\n",
    "            darth_dists_r_target = darth_df[\"dists\"].mean()\n",
    "        \n",
    "            optimal_dists_per_r.append(optimal_dists_r_target)\n",
    "            dists_by_DARTH_per_r.append(darth_dists_r_target)\n",
    "            total_dists_per_r.append(total_dists_r_target)\n",
    "            \n",
    "            dist_diff_ratio = abs(darth_dists_r_target - optimal_dists_r_target) / total_dists_r_target\n",
    "            average_dist_diffs_k_r_target.append(dist_diff_ratio)\n",
    "        \n",
    "        average_dist_diffs_k.append(np.mean(average_dist_diffs_k_r_target))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10,6))\n",
    "        ax.plot(r_targets, [v / 1000 for v in total_dists_per_r], label=\"Total\", color=\"black\", marker=\"^\", linestyle=\"--\", markersize=25, linewidth=3, alpha=0.8)\n",
    "        ax.plot(r_targets, [v / 1000 for v in optimal_dists_per_r], label=\"Optimal\", color=\"gray\", marker=\"s\", linestyle=\"--\", markersize=25, linewidth=3, alpha=0.8)\n",
    "        ax.plot(r_targets, [v / 1000 for v in dists_by_DARTH_per_r], label=\"DARTH\", color=\"tomato\", marker=\"o\", markersize=25, linewidth=3, alpha=0.8)\n",
    "        \n",
    "        # Add a red area between the optimal and DARTH lines\n",
    "        ax.fill_between(r_targets, [v / 1000 for v in optimal_dists_per_r], [v / 1000 for v in dists_by_DARTH_per_r], color=\"red\", alpha=0.2)\n",
    "        \n",
    "        ax.grid(alpha=0.8, linestyle=\"--\")          \n",
    "        ax.set_xlabel(r\"$R_t$\")\n",
    "        ax.set_ylabel(\"Dists \" + r\"($10^{-3}$)\")\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_visible(False)\n",
    "        ax.legend(fontsize=40, frameon=False, loc=\"upper left\", borderpad=0.1, bbox_to_anchor=(0.0, 0.93), handletextpad=0.0, columnspacing=0.5, labelspacing=0.3, handlelength=1.2)\n",
    "        #ax.tick_params(axis='x', labelsize=44)\n",
    "        fig.tight_layout()\n",
    "        filename = f\"{PLOTS_DIR}enhanced_optimality_{ds_name}_k{k}.pdf\"\n",
    "        fig.savefig(filename, bbox_inches='tight')\n",
    "        print(f\"Fig saved at {filename}\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    \n",
    "    print(f\"{k}: Average dist diff ratio: {np.mean(average_dist_diffs_k)}\")\n",
    "    average_dist_diffs.append(np.mean(average_dist_diffs_k))\n",
    "\n",
    "avg_dist_ratio = np.mean(average_dist_diffs)\n",
    "print(f\"Average dist diff ratio: {avg_dist_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\"]#[\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\", \"T2I100M\"]\n",
    "all_k_values = [\"100\", \"75\", \"50\", \"25\", \"10\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\", \"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "# For the queries we surpassed the target recall, find how many distance calculations we did more than needed\n",
    "# For the queries we did not reach the target recall, find how many distance calculations we needed to do more.\n",
    "\n",
    "\n",
    "\n",
    "ds_recalls = {} # k -> dataset -> [recall, std, qut, speedup for each r_target]\n",
    "\n",
    "for k in all_k_values:\n",
    "    ds_recalls[k] = {}\n",
    "    print(f\"k: {k}\")\n",
    "    for ds_name in all_datasets:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        print(f\"    Dataset: {ds_name}\")\n",
    "        ds_recalls[k][ds_name] = []\n",
    "        \n",
    "        for r_target in all_r_targets:\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            \n",
    "            ipi = interval_conf[ds_name][k][r_target][\"ipi\"]\n",
    "            mpi = interval_conf[ds_name][k][r_target][\"mpi\"]\n",
    "            r_target_fl = float(r_target)\n",
    "                \n",
    "            #print(f\"r_target: {r_target}, ipi: {ipi}, mpi: {mpi}\")\n",
    "                \n",
    "            no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "            darth_df = pd.read_csv(get_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k, r_target_fl, ipi, mpi), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",])\n",
    "            no_early_termination_detailed_df = pd.read_csv(get_optimal_early_stop_testing_dataset_name(M, efC, efS, s, ds_name, k), usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"])\n",
    "                    \n",
    "            # for each one of the target recalls, find the minimum dist calcs that we needed to do to reach that recall, and put it in a column named min_dists_XX where XX is the recall\n",
    "            grouped_detailed_no_early_termination = no_early_termination_detailed_df.groupby(\"qid\")\n",
    "            for qid, group in grouped_detailed_no_early_termination:\n",
    "                min_dists = group[group[\"r\"] >= r_target_fl][\"dists\"].min()\n",
    "                min_time = group[group[\"r\"] >= r_target_fl][\"elaps_ms\"].min()\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_dists_{r_target}\"] = min_dists\n",
    "                no_early_termination_df.loc[no_early_termination_df[\"qid\"] == qid, f\"min_time_{r_target}\"] = min_time\n",
    "                #darth_df.loc[darth_df[\"qid\"] == qid, f\"min_dists_{r_target}\"] = min_dists\n",
    "            \n",
    "            # Important:\n",
    "            # For the given recall target, find which queries do not reach the target recall in general (from the no-early-termination runs)\n",
    "            # Then, remove those queries from both the no-early-termination and the DARTH runs\n",
    "            no_early_termination_df = no_early_termination_df[no_early_termination_df[\"r\"] >= r_target_fl]\n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_termination_df[\"qid\"])]\n",
    "            \n",
    "            # Merge darth and no_early_termination_df\n",
    "            rename_dict = {\n",
    "                \"elaps_ms\": \"total_elaps_ms\",\n",
    "                \"dists\": \"total_dists\",\n",
    "                \"inserts\": \"total_inserts\",\n",
    "                \"r\": \"total_r\",\n",
    "                \"step\": \"total_step\",\n",
    "            }\n",
    "            no_early_termination_df = no_early_termination_df.rename(columns=rename_dict)\n",
    "            darth_df = darth_df.merge(no_early_termination_df, on=\"qid\", how=\"inner\")\n",
    "            \n",
    "            # total_dists, min_dists_{r_target}\n",
    "            ratio_of_min_dists_to_total_dists = darth_df[f\"min_dists_{r_target}\"] / darth_df[\"total_dists\"]\n",
    "            avg_ratio = ratio_of_min_dists_to_total_dists.mean()\n",
    "            print(f\"        r_target: {r_target} - Avg ratio: {avg_ratio:.2f}\")\n",
    "            \n",
    "        print()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localPyLibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
