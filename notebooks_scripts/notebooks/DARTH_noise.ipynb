{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with more difficult query workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "from matplotlib.ticker import MaxNLocator  # type: ignore\n",
    "\n",
    "from sklearn.manifold import TSNE  # type: ignore\n",
    "from sklearn.decomposition import PCA  # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler  # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.linear_model import LinearRegression  # type: ignore\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "\n",
    "dataset_params = {\n",
    "        \"SIFT100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"lightblue\",\n",
    "            \"marker\": \"o\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"SIFT100M\"\n",
    "        },\n",
    "        \"DEEP100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 750,\n",
    "            \"color\": \"plum\",\n",
    "            \"marker\": \"x\",  \n",
    "            \"li\": 1 ,\n",
    "            \"label\": \"DEEP100M\"\n",
    "        },\n",
    "        \"T2I100M\": {\n",
    "            \"M\": 80,\n",
    "            \"efC\": 1000,\n",
    "            \"efS\": 2500,\n",
    "            \"color\": \"rosybrown\",\n",
    "            \"marker\": \"d\",\n",
    "            \"li\": 2,\n",
    "            \"label\": \"T2I100M\"\n",
    "        },\n",
    "        \"GLOVE100\": {\n",
    "            \"M\": 16,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"color\": \"orange\",\n",
    "            \"marker\": \"^\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GLOVE1M\"\n",
    "        },\n",
    "        \"GIST1M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 1000,\n",
    "            \"color\": \"lightgreen\",\n",
    "            \"marker\": \"s\",\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GIST1M\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "interval_conf = {}\n",
    "with open(\"../../experiments/generated_json/final_heuristic_adaptive_recommendations_params_ipidiv2_mpidiv10.json\") as f:\n",
    "    interval_conf = json.load(f)\n",
    "\n",
    "laet_multiplier_conf = {}\n",
    "with open(\"../../experiments/generated_json/laet_tuning_results_memoryFalse_validationSize1000.json\") as f:\n",
    "    laet_multiplier_conf = json.load(f)\n",
    "\n",
    "classic_hnsw_conf = {}\n",
    "with open(\"../../experiments/generated_json/classic_hnsw_tuning_results_memoryFalse_validationSize1000.json\") as f:\n",
    "    classic_hnsw_conf = json.load(f)\n",
    "    \n",
    "algorithm_conf = {\n",
    "    \"Plain-HNSW\":{\n",
    "        \"color\": \"red\",\n",
    "        \"marker\": \"D\",\n",
    "        \"label\": \"Plain-HNSW\",\n",
    "    },\n",
    "    \"Naive-Baseline\": {\n",
    "        \"color\": \"black\",\n",
    "        \"marker\": \"s\",\n",
    "        \"label\": \"Baseline\",\n",
    "    },\n",
    "    \"LAET\": {\n",
    "        \"color\": \"gray\",\n",
    "        \"marker\": \"x\",\n",
    "        \"label\": \"LAET\",\n",
    "    },\n",
    "    \"HNSW\": {\n",
    "        \"color\": \"lightgray\",\n",
    "        \"marker\": \"^\",\n",
    "        \"label\": \"REM\",\n",
    "    },\n",
    "    \"DARTH\": {\n",
    "        \"color\": \"tomato\",\n",
    "        \"marker\": \"o\",\n",
    "        \"label\": \"DARTH\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def rqut_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return len(df[df[recall_col] < r_target]) / s\n",
    "\n",
    "def rde_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"RDE\"].mean()\n",
    "\n",
    "def tde_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    new_df = df[df[\"TDR\"] > 0]\n",
    "    return new_df[\"TDR\"].mean()\n",
    "\n",
    "def nrs_rev_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    #return 1/df[\"NRS\"].mean()\n",
    "    return 1/df[df[\"NRS\"] > 0][\"NRS\"].mean()\n",
    "\n",
    "def nrs_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    # remove the negative entries from the df:\n",
    "    new_df = df[df[\"NRS\"] > 0]\n",
    "    return new_df[\"NRS\"].mean()\n",
    "\n",
    "def time_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"elaps_ms\"].mean()\n",
    "\n",
    "def dist_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[\"dists\"].mean()\n",
    "\n",
    "def p99_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] <= r_target]\n",
    "    error = np.abs(below_df[recall_col] - r_target)\n",
    "    return np.percentile(error, 99)\n",
    "\n",
    "def error_mean_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] < r_target]    \n",
    "    return below_df[recall_col].mean()\n",
    "\n",
    "def worst_errors_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    below_df = df[df[recall_col] < r_target]    \n",
    "    return below_df[recall_col].max()\n",
    "\n",
    "def worst_1perc_errors_mean_calculation(df, s, r_target, recall_col=\"r\"):\n",
    "    # Filter entries below the target recall\n",
    "    below_df = df[df[recall_col] < r_target].copy()\n",
    "    \n",
    "    # Calculate the error as the absolute difference\n",
    "    below_df[\"error\"] = abs(r_target - below_df[recall_col])\n",
    "    \n",
    "    # Sort the DataFrame by the error in descending order\n",
    "    sorted_below_df = below_df.sort_values(by=\"error\", ascending=False)\n",
    "    \n",
    "    # Calculate the top 1% of the worst errors\n",
    "    num_entries = len(sorted_below_df)\n",
    "    worst_1_percent_count = max(1, int(0.01 * num_entries))  # At least 1 entry\n",
    "    \n",
    "    # Select the worst 1% errors and calculate their mean\n",
    "    worst_1_perc_mean = sorted_below_df[\"error\"].head(worst_1_percent_count).mean()\n",
    "    \n",
    "    return worst_1_perc_mean\n",
    "\n",
    "def qps_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    average_time_ms = df[\"elaps_ms\"].mean()\n",
    "    average_time_seconds = average_time_ms / 1000\n",
    "    qps = 1 / average_time_seconds\n",
    "    return qps\n",
    "\n",
    "def recall_calculator(df, s, r_target, recall_col=\"r\"):\n",
    "    return df[recall_col].mean()\n",
    "\n",
    "quality_measures = {\n",
    "    \"RQUT\": {\n",
    "        \"label\": \"RQUT\",\n",
    "        \"calculator\": rqut_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"RDE\": {\n",
    "        \"label\": \"RDE\",\n",
    "        \"calculator\": rde_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"TDR\": {\n",
    "        \"label\": \"TDR\",\n",
    "        \"calculator\": tde_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \n",
    "    \"NRS_rev\": {\n",
    "        \"label\": r\"$NRS^{-1}$\",\n",
    "        \"calculator\": nrs_rev_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"NRS\": {\n",
    "        \"label\": \"NRS\",\n",
    "        \"calculator\": nrs_calculator,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"Time\": {\n",
    "        \"label\": \"Time (ms)\",\n",
    "        \"calculator\": time_calculator,\n",
    "        \"type\": \"line\"\n",
    "    },\n",
    "    \"Dist\": {\n",
    "        \"label\": \"Dists\",\n",
    "        \"calculator\": dist_calculation,\n",
    "        \"type\": \"line\"\n",
    "    },\n",
    "    \"p99\": {\n",
    "        \"label\": r\"$P99$\",\n",
    "        \"calculator\": p99_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"ErrorMean\": {\n",
    "        \"label\": \"Error Mean\",\n",
    "        \"calculator\": error_mean_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"WorstErrors\": {\n",
    "        \"label\": \"Worst Errors\",\n",
    "        \"calculator\": worst_errors_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"Worst1perc\": {\n",
    "        \"label\": \"Worst 1%\",\n",
    "        \"calculator\": worst_1perc_errors_mean_calculation,\n",
    "        \"type\": \"bar\"\n",
    "    },\n",
    "    \"QPS\": {\n",
    "        \"label\": \"QPS\",\n",
    "        \"calculator\": qps_calculator,\n",
    "        \"type\": \"line\"\n",
    "    },\n",
    "    \"Recall\": {\n",
    "        \"label\": \"Recall\",\n",
    "        \"calculator\": recall_calculator,\n",
    "        \"type\": \"bar\",\n",
    "        \"label\": \"Actual Recall\",\n",
    "    }\n",
    "}\n",
    "\n",
    "PLOTS_DIR = \"./../../experiments/revision-plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying noise experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy experiment:\n",
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]#, \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "s = 1000\n",
    "\n",
    "# keep the gtaph without the recall\n",
    "# check if there is evident diff with the results of the original paper\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "\n",
    "all_noise_percentages = [\"1\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\", \"14\", \"16\", \"18\", \"20\", \"22\", \"24\", \"26\", \"28\", \"30\"]\n",
    "\n",
    "all_r_targets = [\"0.9\"]\n",
    "all_measures = [\"Recall\"]\n",
    "for measure in all_measures:\n",
    "    measure_label = quality_measures[measure][\"label\"]\n",
    "    measure_calculator = quality_measures[measure][\"calculator\"]\n",
    "    for ds_name in all_datasets:\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        for k in all_k_values:\n",
    "            for r_target in all_r_targets:\n",
    "                fig, ax = plt.subplots(figsize=(10,6))\n",
    "                \n",
    "                ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "                f_r_target = float(r_target)\n",
    "                \n",
    "                plain_HNSW_values_per_noise = []\n",
    "                \n",
    "                DARTH_values_per_noise = []\n",
    "                LAET_values_per_noise = []\n",
    "                HNSW_values_per_noise = []\n",
    "                naive_values_per_noise = []\n",
    "                \n",
    "                for noise_percentage in all_noise_percentages:\n",
    "                    no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    if int(noise_percentage) == 12:\n",
    "                        no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target-0.05]                \n",
    "                    \n",
    "                    average_no_early_stop_recall = no_early_stop_df[\"r\"].mean()\n",
    "                    print(f\"Average recall for {ds_name} with noise {noise_percentage} is {average_no_early_stop_recall} for {len(no_early_stop_df)} queries\")\n",
    "                    \n",
    "                    darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    naive_df = pd.read_csv(f\"../../experiments/results/naive-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    laet_df  = pd.read_csv(f\"../../experiments/results/laet-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    cefs = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "                    hnsw_df  = pd.read_csv(f\"../../experiments/results/classic-hnsw/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{cefs}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                        \n",
    "                    darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    naive_df = naive_df[naive_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    laet_df = laet_df[laet_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "\n",
    "                    measure_plain_HNSW = measure_calculator(no_early_stop_df, s, f_r_target)\n",
    "                    measure_darth = measure_calculator(darth_df, s, f_r_target, recall_col=\"r_actual\")\n",
    "                    measure_laet = measure_calculator(laet_df, s, f_r_target)\n",
    "                    measure_naive = measure_calculator(naive_df, s, f_r_target)\n",
    "                    measure_hnsw = measure_calculator(hnsw_df, s, f_r_target)\n",
    "\n",
    "                    plain_HNSW_values_per_noise.append(measure_plain_HNSW)\n",
    "                    DARTH_values_per_noise.append(measure_darth)    \n",
    "                    LAET_values_per_noise.append(measure_laet)\n",
    "                    naive_values_per_noise.append(measure_naive)\n",
    "                    HNSW_values_per_noise.append(measure_hnsw)\n",
    "                \n",
    "                x = [int(noise_perc) for noise_perc in all_noise_percentages]\n",
    "                markersize = 25\n",
    "                linewidth = 3\n",
    "                ax.plot(x, naive_values_per_noise, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], marker=algorithm_conf[\"Naive-Baseline\"][\"marker\"], label=algorithm_conf[\"Naive-Baseline\"][\"label\"], markersize=markersize, linewidth=linewidth)\n",
    "                ax.plot(x, HNSW_values_per_noise, color=algorithm_conf[\"HNSW\"][\"color\"], marker=algorithm_conf[\"HNSW\"][\"marker\"], label=algorithm_conf[\"HNSW\"][\"label\"], markersize=markersize, linewidth=linewidth)\n",
    "                ax.plot(x, LAET_values_per_noise, color=algorithm_conf[\"LAET\"][\"color\"], marker=algorithm_conf[\"LAET\"][\"marker\"], label=algorithm_conf[\"LAET\"][\"label\"], markersize=markersize, linewidth=linewidth)\n",
    "                ax.plot(x, DARTH_values_per_noise, color=algorithm_conf[\"DARTH\"][\"color\"], marker=algorithm_conf[\"DARTH\"][\"marker\"], label=algorithm_conf[\"DARTH\"][\"label\"], markersize=markersize, linewidth=linewidth)\n",
    "                \n",
    "                \n",
    "                ax.plot(x, plain_HNSW_values_per_noise, color=algorithm_conf[\"Plain-HNSW\"][\"color\"], label=algorithm_conf[\"Plain-HNSW\"][\"label\"], markersize=15, linewidth=linewidth, linestyle=\"--\", alpha=0.5)\n",
    "                ax.fill_between(x, plain_HNSW_values_per_noise, max(plain_HNSW_values_per_noise), color=\"darkgray\", alpha=0.5)\n",
    "                \n",
    "                # fill between the plain HNSW and the DARTH\n",
    "                ax.fill_between(x, plain_HNSW_values_per_noise, DARTH_values_per_noise, color=\"red\", alpha=0.2)\n",
    "                \n",
    "                ax.grid(alpha=0.8, linestyle=\"--\")\n",
    "                \n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "\n",
    "                ax.set_xlabel(\"Noise %\")\n",
    "                ax.set_ylabel(measure_label)\n",
    "                \n",
    "                # make y-axis log scale\n",
    "                #ax.set_yscale(\"log\")\n",
    "                \n",
    "                ticks = ax.get_yticks()\n",
    "                ticks = [0.0, 0.5, 1.0]#f_r_target]\n",
    "                \n",
    "                ax.set_yticks(sorted(ticks))\n",
    "                ax.set_ylim(bottom=0, top=1.01)\n",
    "                \n",
    "                # set x-axis labels to be the recall levels\n",
    "                ax.set_xticks([1, 10, 20, 30])\n",
    "                \n",
    "                fig.tight_layout()\n",
    "                filename=f\"{PLOTS_DIR}noise_{measure}_{ds_name}_k{k}_rt{r_target}.pdf\"\n",
    "                fig.savefig(filename, format=\"pdf\", bbox_inches = 'tight')\n",
    "                print(f\"Saved figure to {filename}\")\n",
    "                \n",
    "                plt.show()\n",
    "                plt.close(fig)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Quality Measures with static noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]#[\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\"]# \"0.95\"]#, \"0.99\"]\n",
    "all_measures = [\"RDE\", \"RQUT\", \"NRS_rev\", \"p99\", \"Worst1perc\",]#[\"NRS_rev\", \"p99\", \"RDE\", \"RQUT\", \"Worst1perc\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "bar_width = 0.2\n",
    "s = 1000\n",
    "\n",
    "for noise_percentage in [\"12\"]:\n",
    "    for measure in all_measures:\n",
    "        measure_label = quality_measures[measure][\"label\"]\n",
    "        measure_calculator = quality_measures[measure][\"calculator\"]\n",
    "        \n",
    "        average_measure_improvement_DARTH_vs_LAET = 0\n",
    "        average_measure_improvement_DARTH_vs_HNSW = 0\n",
    "        average_measure_improvement_DARTH_vs_baseline = 0\n",
    "        average_measure_calc_num = 0\n",
    "        \n",
    "        for i, ds_name in enumerate(all_datasets):\n",
    "            M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "            naive_values, DARTH_values, LAET_values, HNSW_values = [], [], [], []\n",
    "            naive_errors, DARTH_errors, LAET_errors, HNSW_errors = [], [], [], []  # For storing standard errors\n",
    "            \n",
    "            for r_target in all_r_targets:\n",
    "                DARTH_values_k = []\n",
    "                LAET_values_k = []\n",
    "                naive_values_k = []\n",
    "                HNSW_values_k = []\n",
    "                \n",
    "                for k in all_k_values:\n",
    "                    ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "                    f_r_target = float(r_target)\n",
    "                    \n",
    "                    no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]              \n",
    "                    \n",
    "                    darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    naive_df = pd.read_csv(f\"../../experiments/results/naive-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    laet_df  = pd.read_csv(f\"../../experiments/results/laet-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    cefs = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "                    hnsw_df  = pd.read_csv(f\"../../experiments/results/classic-hnsw/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{cefs}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                                \n",
    "                    darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    naive_df = naive_df[naive_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    laet_df = laet_df[laet_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    \n",
    "                    #print(f\"DARTH average recall: {darth_df['r_actual'].mean()}\")\n",
    "                    \n",
    "                    measure_darth = measure_calculator(darth_df, s, f_r_target, recall_col=\"r_actual\")\n",
    "                    measure_laet = measure_calculator(laet_df, s, f_r_target)\n",
    "                    measure_naive = measure_calculator(naive_df, s, f_r_target)\n",
    "                    measure_hnsw = measure_calculator(hnsw_df, s, f_r_target)\n",
    "                    \n",
    "                    #print(f\"Measure {measure} for {ds_name} with noise {noise_percentage} and k {k} is {measure_darth}\")\n",
    "                    \n",
    "                    DARTH_values_k.append(measure_darth)\n",
    "                    LAET_values_k.append(measure_laet)\n",
    "                    naive_values_k.append(measure_naive)\n",
    "                    HNSW_values_k.append(measure_hnsw)\n",
    "                    \n",
    "                    # Calculate relative improvement\n",
    "                    average_measure_improvement_DARTH_vs_LAET += measure_laet / (measure_darth + 1e-6)\n",
    "                    average_measure_improvement_DARTH_vs_baseline += measure_naive / (measure_darth + 1e-6)\n",
    "                    average_measure_improvement_DARTH_vs_HNSW += measure_hnsw / (measure_darth + 1e-6)\n",
    "                    average_measure_calc_num += 1\n",
    "\n",
    "                naive_values.append(np.mean(naive_values_k))\n",
    "                DARTH_values.append(np.mean(DARTH_values_k))\n",
    "                LAET_values.append(np.mean(LAET_values_k))\n",
    "                HNSW_values.append(np.mean(HNSW_values_k))\n",
    "\n",
    "                # Calculate standard error for each set of values\n",
    "                naive_errors.append(np.std(naive_values_k) / np.sqrt(len(naive_values_k)))\n",
    "                DARTH_errors.append(np.std(DARTH_values_k) / np.sqrt(len(DARTH_values_k)))\n",
    "                LAET_errors.append(np.std(LAET_values_k) / np.sqrt(len(LAET_values_k)))\n",
    "                HNSW_errors.append(np.std(HNSW_values_k) / np.sqrt(len(HNSW_values_k)))\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "            x = np.arange(len(all_r_targets))\n",
    "            \n",
    "            if quality_measures[measure][\"type\"] == \"bar\":\n",
    "                #ax.bar(x - 1.5*bar_width, naive_values, width=bar_width, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, yerr=naive_errors, capsize=5)\n",
    "                #ax.bar(x - 0.5*bar_width, LAET_values, width=bar_width, color=algorithm_conf[\"LAET\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, yerr=LAET_errors, capsize=5)\n",
    "                #ax.bar(x + 0.5*bar_width, HNSW_values, width=bar_width, color=algorithm_conf[\"HNSW\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, yerr=HNSW_errors, capsize=5)\n",
    "                #ax.bar(x + 1.5*bar_width, DARTH_values, width=bar_width, color=algorithm_conf[\"DARTH\"][\"color\"], alpha=1, edgecolor=\"black\", linewidth=3, yerr=DARTH_errors, capsize=5)\n",
    "                \n",
    "                ax.bar(x - 1.5*bar_width, naive_values, width=bar_width, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "                ax.bar(x - 0.5*bar_width, LAET_values, width=bar_width, color=algorithm_conf[\"LAET\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "                ax.bar(x + 0.5*bar_width, HNSW_values, width=bar_width, color=algorithm_conf[\"HNSW\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "                ax.bar(x + 1.5*bar_width, DARTH_values, width=bar_width, color=algorithm_conf[\"DARTH\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "                \n",
    "                ax.grid(alpha=0.8, axis=\"y\", linestyle=\"--\")\n",
    "            else:\n",
    "                ax.plot(x, naive_values, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], marker=algorithm_conf[\"Naive-Baseline\"][\"marker\"], label=algorithm_conf[\"Naive-Baseline\"][\"label\"], markersize=15)\n",
    "                ax.plot(x, HNSW_values, color=algorithm_conf[\"HNSW\"][\"color\"], marker=algorithm_conf[\"HNSW\"][\"marker\"], label=algorithm_conf[\"HNSW\"][\"label\"], markersize=15)\n",
    "                ax.plot(x, LAET_values, color=algorithm_conf[\"LAET\"][\"color\"], marker=algorithm_conf[\"LAET\"][\"marker\"], label=algorithm_conf[\"LAET\"][\"label\"], markersize=15)\n",
    "                ax.plot(x, DARTH_values, color=algorithm_conf[\"DARTH\"][\"color\"], marker=algorithm_conf[\"DARTH\"][\"marker\"], label=algorithm_conf[\"DARTH\"][\"label\"], markersize=15)\n",
    "                ax.grid(alpha=0.8, linestyle=\"--\")\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            \n",
    "            ax.set_xticks(np.arange(len(all_r_targets)))\n",
    "            #ax.set_xticklabels(all_r_targets)\n",
    "            ax.set_xticklabels([\"0.80\", \"0.85\", \"0.90\"])#, \"0.95\"])\n",
    "            ax.set_xlabel(r\"$R_t$\")\n",
    "            ax.set_ylabel(f\"{measure_label}\")\n",
    "            #ax.legend()\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            \n",
    "            if len(all_k_values) == 1:\n",
    "                savepath = f\"{PLOTS_DIR}noisy{noise_percentage}_comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\"\n",
    "                #fig.savefig(f\"../../experiments/plots/enhanced_noisy{noise_percentage}_comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\", bbox_inches=\"tight\")\n",
    "                #print(f\"Saved plot ./../../experiments/plots/enhanced_noisy{noise_percentage}_comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\")\n",
    "                #fig.savefig(f\"../../experiments/plots/enhanced_noisy{noise_percentage}_comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\", bbox_inches=\"tight\")\n",
    "                #print(f\"Saved plot ./../../experiments/plots/enhanced_noisy{noise_percentage}_comparisons_{measure}_k{all_k_values[0]}_{ds_name}.pdf\")\n",
    "            else:\n",
    "                savepath = f\"{PLOTS_DIR}noisy{noise_percentage}_comparisons_{measure}_{ds_name}.pdf\"\n",
    "                #fig.savefig(f\"../../experiments/plots/noisy{noise_percentage}_comparisons_{measure}_{ds_name}.pdf\", bbox_inches=\"tight\")\n",
    "                #print(f\"Saved plot ./../../experiments/plots/noisy{noise_percentage}_comparisons_{measure}_{ds_name}.pdf\")\n",
    "            fig.savefig(savepath, format=\"pdf\", bbox_inches = 'tight', pad_inches=0)\n",
    "            #print(f\"Saved plot to {savepath}\")\n",
    "            \n",
    "            #plt.show()\n",
    "            plt.close(fig)\n",
    "        \n",
    "        #print(f\"Average {measure} improvement of DARTH vs LAET: {average_measure_improvement_DARTH_vs_LAET / average_measure_calc_num:.2f}\")\n",
    "        #print(f\"Average {measure} improvement of DARTH vs HNSW: {average_measure_improvement_DARTH_vs_HNSW / average_measure_calc_num:.2f}\")\n",
    "        #print(f\"Average {measure} improvement of DARTH vs Baseline: {average_measure_improvement_DARTH_vs_baseline / average_measure_calc_num:.2f}\")\n",
    "        \n",
    "        improvement_DARTH_vs_LAET = average_measure_improvement_DARTH_vs_LAET / average_measure_calc_num\n",
    "        improvement_DARTH_vs_HNSW = average_measure_improvement_DARTH_vs_HNSW / average_measure_calc_num\n",
    "        improvement_DARTH_vs_baseline = average_measure_improvement_DARTH_vs_baseline / average_measure_calc_num\n",
    "        \n",
    "        perc_improvement_DARTH_vs_LAET = (improvement_DARTH_vs_LAET - 1) * 100\n",
    "        perc_improvement_DARTH_vs_HNSW = (improvement_DARTH_vs_HNSW - 1) * 100\n",
    "        perc_improvement_DARTH_vs_baseline = (improvement_DARTH_vs_baseline - 1) * 100\n",
    "        \n",
    "        print(f\"Average {measure} improvement of DARTH vs LAET: {perc_improvement_DARTH_vs_LAET:.0f}%\")\n",
    "        print(f\"Average {measure} improvement of DARTH vs HNSW: {perc_improvement_DARTH_vs_HNSW:.0f}%\")\n",
    "        print(f\"Average {measure} improvement of DARTH vs Baseline: {perc_improvement_DARTH_vs_baseline:.0f}%\")\n",
    "        \n",
    "        average_perc_improvement_overall = (perc_improvement_DARTH_vs_LAET + perc_improvement_DARTH_vs_HNSW + perc_improvement_DARTH_vs_baseline) / 3\n",
    "        print(f\"Average percentage improvement of DARTH over all algorithms: {average_perc_improvement_overall:.0f}%\")\n",
    "        \n",
    "        print()   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Time Performance Measures with static nosie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 10 with noise\n",
    "# QPS graphs but with the actual achieved recall (not the recall target)\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\", \"T2I100M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\"]\n",
    "all_measures = [\"QPS\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 40})\n",
    "bar_width = 0.2\n",
    "\n",
    "\n",
    "for noise_percentage in [\"12\"]:\n",
    "    for measure in all_measures:\n",
    "        measure_label = quality_measures[measure][\"label\"]\n",
    "        measure_calculator = quality_measures[measure][\"calculator\"]\n",
    "        \n",
    "        for i, ds_name in enumerate(all_datasets):\n",
    "            M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "            naive_values, DARTH_values, LAET_values, HNSW_values = [], [], [], []\n",
    "            naive_values_achieved_recalls, DARTH_values_achieved_recalls, LAET_values_achieved_recalls, HNSW_values_achieved_recalls = [], [], [], []\n",
    "\n",
    "            for r_target in all_r_targets:\n",
    "                DARTH_values_k = []\n",
    "                LAET_values_k = []\n",
    "                naive_values_k = []\n",
    "                HNSW_values_k = []\n",
    "                \n",
    "                DARTH_values_k_achieved_recalls = []\n",
    "                LAET_values_k_achieved_recalls = []\n",
    "                naive_values_k_achieved_recalls = []\n",
    "                HNSW_values_k_achieved_recalls = []\n",
    "                \n",
    "                for k in all_k_values:\n",
    "                    ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "                    f_r_target = float(r_target)\n",
    "                    \n",
    "                    no_early_stop_df = pd.read_csv(f\"../results/no-early-stop/testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]                \n",
    "                            \n",
    "                    darth_df = pd.read_csv(f\"../results/early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    naive_df = pd.read_csv(f\"../results/naive-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    laet_df  = pd.read_csv(f\"../results/laet-early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"predicted_distance_calcs\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                    cefs = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "                    hnsw_df  = pd.read_csv(f\"../results/classic-hnsw/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{cefs}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                            \n",
    "                    darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    naive_df = naive_df[naive_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    laet_df = laet_df[laet_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    \n",
    "                    achieved_recall_darth = darth_df[\"r_actual\"].mean()\n",
    "                    achieved_recall_naive = naive_df[\"r\"].mean()\n",
    "                    achieved_recall_laet = laet_df[\"r\"].mean()\n",
    "                    achieved_recall_hnsw = hnsw_df[\"r\"].mean()\n",
    "                    \n",
    "                    measure_darth = measure_calculator(darth_df, s, f_r_target, recall_col=\"r_actual\")\n",
    "                    measure_laet = measure_calculator(laet_df, s, f_r_target)\n",
    "                    measure_naive = measure_calculator(naive_df, s, f_r_target)\n",
    "                    measure_hnsw = measure_calculator(hnsw_df, s, f_r_target)\n",
    "                    \n",
    "                    DARTH_values_k.append(measure_darth)\n",
    "                    LAET_values_k.append(measure_laet)\n",
    "                    naive_values_k.append(measure_naive)\n",
    "                    HNSW_values_k.append(measure_hnsw)\n",
    "                    \n",
    "                    DARTH_values_k_achieved_recalls.append(achieved_recall_darth)\n",
    "                    LAET_values_k_achieved_recalls.append(achieved_recall_laet)\n",
    "                    naive_values_k_achieved_recalls.append(achieved_recall_naive)\n",
    "                    HNSW_values_k_achieved_recalls.append(achieved_recall_hnsw)\n",
    "                    \n",
    "                    #print(f\"ds={ds_name} k={k} rt={r_target} meas={measure_label} || DARTH: {measure_darth} LAET: {measure_laet} Naive: {measure_naive}\")\n",
    "                \n",
    "                naive_values.append(np.mean(naive_values_k))\n",
    "                DARTH_values.append(np.mean(DARTH_values_k))\n",
    "                LAET_values.append(np.mean(LAET_values_k))\n",
    "                HNSW_values.append(np.mean(HNSW_values_k))\n",
    "                \n",
    "                naive_values_achieved_recalls.append(np.mean(naive_values_k_achieved_recalls))\n",
    "                DARTH_values_achieved_recalls.append(np.mean(DARTH_values_k_achieved_recalls))\n",
    "                LAET_values_achieved_recalls.append(np.mean(LAET_values_k_achieved_recalls))\n",
    "                HNSW_values_achieved_recalls.append(np.mean(HNSW_values_k_achieved_recalls))\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "            x = np.arange(len(all_r_targets))\n",
    "            \n",
    "            ax.plot(naive_values_achieved_recalls, naive_values, color=algorithm_conf[\"Naive-Baseline\"][\"color\"], marker=algorithm_conf[\"Naive-Baseline\"][\"marker\"], label=algorithm_conf[\"Naive-Baseline\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "            ax.plot(HNSW_values_achieved_recalls, HNSW_values, color=algorithm_conf[\"HNSW\"][\"color\"], marker=algorithm_conf[\"HNSW\"][\"marker\"], label=algorithm_conf[\"HNSW\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "            ax.plot(LAET_values_achieved_recalls, LAET_values, color=algorithm_conf[\"LAET\"][\"color\"], marker=algorithm_conf[\"LAET\"][\"marker\"], label=algorithm_conf[\"LAET\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "            ax.plot(DARTH_values_achieved_recalls, DARTH_values, color=algorithm_conf[\"DARTH\"][\"color\"], marker=algorithm_conf[\"DARTH\"][\"marker\"], label=algorithm_conf[\"DARTH\"][\"label\"], markersize=30, linewidth=3, markeredgewidth=3)\n",
    "            ax.grid(alpha=0.8, linestyle=\"--\")\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "\n",
    "            #ax.set_xticks([0.80, 0.85, 0.90, 0.95, 1])\n",
    "            ax.set_xlabel(\"Actual Recall\")\n",
    "            ax.set_ylabel(f\"{measure_label}\")\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            if len(all_k_values) == 1:\n",
    "                filename = f\"./plots/noisy{noise_percentage}_comparisons_actual_recall_{measure}_k{all_k_values[0]}_{ds_name}.pdf\"\n",
    "            else:\n",
    "                filename = f\"./plots/noisy{noise_percentage}_comparisons_actual_recall_{measure}_{ds_name}.pdf\"\n",
    "            fig.savefig(filename, bbox_inches=\"tight\")\n",
    "\n",
    "            print(f\"Saved plot at {filename}\")\n",
    "            #plt.show()\n",
    "\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison times with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 10 with noise\n",
    "# QPS graphs but with the actual achieved recall (not the recall target)\n",
    "\n",
    "# todo here: find for 12% noise which multiplier/efsearch value gets the average recall to 0.9 for LAET and REM\n",
    "\n",
    "multiplier_for_rt = {\n",
    "    \"SIFT100M\": 0.5,\n",
    "}\n",
    "efsearch_for_rt = {\n",
    "}\n",
    "dists_for_rt = {\n",
    "    \n",
    "}\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]#, \"DEEP100M\", \"GLOVE100\", \"GIST1M\", \"T2I100M\"]\n",
    "all_dataset_labels = [\"SIFT100M\", \"DEEP100M\", \"GLOVE1M\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.9\"]\n",
    "all_measures = [\"QPS\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 48})\n",
    "bar_width = 0.2\n",
    "\n",
    "r_target = \"0.9\"\n",
    "k = \"50\"\n",
    "\n",
    "for noise_percentage in [\"12\"]:\n",
    "    hnsw_QPS = []\n",
    "    DARTH_QPS = []\n",
    "    DARTH_speedups = []\n",
    "    \n",
    "    average_QPS_more_DARTH = 0\n",
    "    average_speedup_more_DARTH = 0\n",
    "    \n",
    "    max_QPS_more_DARTH = 0\n",
    "    max_speedup_more_DARTH = 0\n",
    "    \n",
    "    total_runs = 0\n",
    "        \n",
    "    for i, ds_name in enumerate(all_datasets):\n",
    "        M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "\n",
    "        ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "        f_r_target = float(r_target)\n",
    "                    \n",
    "        no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "        achieved_plain_HNSW_recall = no_early_stop_df[\"r\"].mean()\n",
    "        \n",
    "        no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target] # check this again and make sure no problems are caused by removing queries\n",
    "        achieved_plain_HNSW_recall_1 = no_early_stop_df[\"r\"].mean()           \n",
    "                       \n",
    "        darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "        darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    \n",
    "        achieved_recall_darth = darth_df[\"r_actual\"].mean()\n",
    "                    \n",
    "        avg_no_early_term_query_time = no_early_stop_df[\"elaps_ms\"].mean()\n",
    "        avg_darth_query_time = darth_df[\"elaps_ms\"].mean()\n",
    "                    \n",
    "        QPS_darth = 1/(avg_darth_query_time/1000)\n",
    "        QPS_no_early_term = 1/(avg_no_early_term_query_time/1000)\n",
    "                \n",
    "        per_query_speedups = no_early_stop_df[\"elaps_ms\"]/darth_df[\"elaps_ms\"]\n",
    "        DARTH_speedup = per_query_speedups.mean() \n",
    "        \n",
    "        hnsw_QPS.append(QPS_no_early_term)\n",
    "        DARTH_QPS.append(QPS_darth)\n",
    "        DARTH_speedups.append(DARTH_speedup)\n",
    "        \n",
    "        #print(achieved_plain_HNSW_recall, achieved_plain_HNSW_recall_1, achieved_recall_darth)\n",
    "        \n",
    "        average_QPS_more_DARTH += QPS_darth - QPS_no_early_term\n",
    "        average_speedup_more_DARTH += DARTH_speedup\n",
    "        total_runs += 1\n",
    "        \n",
    "        max_QPS_more_DARTH = max(max_QPS_more_DARTH, QPS_darth - QPS_no_early_term)\n",
    "        max_speedup_more_DARTH = max(max_speedup_more_DARTH, DARTH_speedup)\n",
    "    \n",
    "    print(f\"Average QPS more DARTH: {average_QPS_more_DARTH / total_runs:.0f}\")\n",
    "    print(f\"Average speedup more DARTH: {average_speedup_more_DARTH / total_runs:.2f}\")\n",
    "    print(f\"Max QPS more DARTH: {max_QPS_more_DARTH:.0f}\")\n",
    "    print(f\"Max speedup more DARTH: {max_speedup_more_DARTH:.2f}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    \n",
    "    x = np.arange(len(all_datasets))\n",
    "    ax.bar(x - 0.5*bar_width, hnsw_QPS, width=bar_width, color=algorithm_conf[\"HNSW\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, label=\"HNSW\", capsize=5)\n",
    "    ax.bar(x + 0.5*bar_width, DARTH_QPS, width=bar_width, color=algorithm_conf[\"DARTH\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, label=\"DARTH\", capsize=5)\n",
    "    ax.grid(alpha=0.8, axis=\"y\", linestyle=\"--\")\n",
    "    \n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "            \n",
    "    ax.set_xticks(np.arange(len(all_datasets)))\n",
    "    ax.set_xticklabels(all_dataset_labels, rotation=15)\n",
    "    plt.xticks(fontsize=36)\n",
    "    ax.set_ylabel(f\"QPS/Core\")\n",
    "    # set y ticks to be 0, 100, 200, 300, 400\n",
    "    ticks = ax.get_yticks()\n",
    "    ticks = [0, 200, 400, 600]\n",
    "    ax.set_yticks(sorted(ticks))\n",
    "            \n",
    "    fig.savefig(f\"{PLOTS_DIR}noisy{noise_percentage}_QPS_rt{r_target}_k{k}.pdf\", bbox_inches=\"tight\")\n",
    "    print(f\"Saved plot at {PLOTS_DIR}noisy{noise_percentage}_QPS_rt{r_target}_k{k}.pdf\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    #fig, ax = plt.subplots(figsize=(10,6))\n",
    "    #x = np.arange(len(all_datasets))\n",
    "    #ax.bar(x, DARTH_speedups, width=bar_width, color=algorithm_conf[\"DARTH\"][\"color\"], alpha=0.8, edgecolor=\"black\", linewidth=2, label=\"DARTH\", capsize=5)\n",
    "    #ax.grid(alpha=0.8, axis=\"y\", linestyle=\"--\")\n",
    "    \n",
    "    #for spine in ax.spines.values():\n",
    "    #    spine.set_visible(False)\n",
    "            \n",
    "    #ax.set_xticks(np.arange(len(all_datasets)))\n",
    "    #ax.set_xticklabels(all_dataset_labels, rotation=45)\n",
    "    #plt.xticks(fontsize=40)\n",
    "    #ax.set_ylabel(f\"Times Faster\")\n",
    "    # start y-axis at 1\n",
    "    #ax.set_ylim(bottom=1)\n",
    "            \n",
    "    #fig.savefig(f\"./plots/noisy{noise_percentage}_speedup_rt{r_target}_k{k}.pdf\", bbox_inches=\"tight\")\n",
    "    #print(f\"Saved plot at ./plots/noisy{noise_percentage}_speedup_rt{r_target}_k{k}.pdf\")\n",
    "    \n",
    "    #plt.show()\n",
    "    #plt.close(fig)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revision plan histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"DEEP100M\"]#[\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]#[\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.95\"]# \"0.95\"]#, \"0.99\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 34})\n",
    "bar_width = 0.2\n",
    "s = 1000\n",
    "\n",
    "\n",
    "for i, ds_name in enumerate(all_datasets):\n",
    "    M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "   \n",
    "    for r_target in all_r_targets:\n",
    "                \n",
    "        for k in all_k_values:\n",
    "            ipi, mpi = interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "            f_r_target = float(r_target)\n",
    "                    \n",
    "            no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]              \n",
    "                    \n",
    "            cefs = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "            \n",
    "            noise_percentage = \"0\"\n",
    "            if noise_percentage != \"0\":\n",
    "                darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "                hnsw_df  = pd.read_csv(f\"../../experiments/results/classic-hnsw/{ds_name}/k{k}/noisy/noise{noise_percentage}_M{M}_efC{efC}_efS{cefs}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            else:\n",
    "                darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])        \n",
    "                hnsw_df  = pd.read_csv(f\"../../experiments/results/classic-hnsw/{ds_name}/k{k}/M{M}_efC{efC}_efS{cefs}_qs{s}_tr{f_r_target:.2f}.txt\", usecols=[\"qid\", \"elaps_ms\", \"dists\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            \n",
    "            # how many queries of DARTH have recall 1.00\n",
    "            #darth_df = darth_df[darth_df[\"r_actual\"] == 1.00]\n",
    "            #print(len(darth_df))\n",
    "            \n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "            hnsw_df = hnsw_df[hnsw_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    \n",
    "            recall_darth = darth_df[\"r_actual\"].mean()\n",
    "            recall_hnsw = hnsw_df[\"r\"].mean()\n",
    "            \n",
    "            speedup_vs_no_early_stop = no_early_stop_df[\"elaps_ms\"].mean() / darth_df[\"elaps_ms\"].mean()\n",
    "            print(f\"Speedup vs no early stop: {speedup_vs_no_early_stop}\")\n",
    "            \n",
    "            print(f\"Recall DARTH: {recall_darth} Recall HNSW: {recall_hnsw}\")\n",
    "            \n",
    "            darth_rqt = darth_df[darth_df[\"r_actual\"] < f_r_target].shape[0]\n",
    "            hnsw_rqt = hnsw_df[hnsw_df[\"r\"] < f_r_target].shape[0]\n",
    "            \n",
    "            print(f\"Queries DARTH: {darth_rqt} Queries HNSW: {hnsw_rqt}\")\n",
    "            print(f\"Queries DARTH: {darth_rqt}\")\n",
    "\n",
    "            # what is the minimum recall of HNSW?\n",
    "            #print(f\"Minimum recall of HNSW: {hnsw_df['r'].min()}\")\n",
    "            print(f\"Minimum recall of DARTH: {darth_df['r_actual'].min()}\")\n",
    "            \n",
    "            # how many queries per HNSW bin?\n",
    "            #print(hnsw_df[\"r\"].value_counts())\n",
    "            #print(darth_df[\"r_actual\"].value_counts())\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(9,5))\n",
    "            ax.hist(hnsw_df[\"r\"], alpha=0.9, label=\"\", color=algorithm_conf[\"HNSW\"][\"color\"], linewidth=2, edgecolor=\"black\", bins=20)\n",
    "            ax.hist(darth_df[\"r_actual\"], alpha=0.9, label=\"\", color=algorithm_conf[\"DARTH\"][\"color\"], linewidth=2, edgecolor=\"black\", bins=7)\n",
    "            ax.axvline(x=f_r_target, color=\"red\", linestyle=\"--\", label=f\"Rt={f_r_target}\", linewidth=3)\n",
    "            ax.legend(fontsize=28)\n",
    "            ax.set_xlabel(\"Recall\")\n",
    "            ax.set_ylabel(\"# Queries (log)\", fontsize=28)\n",
    "            ax.grid(alpha=0.7, linestyle=\"--\")    \n",
    "                \n",
    "            # protasi gia HNSW 0.99\n",
    "            # ti recall kai speedup exei o DARTH 1\n",
    "            # kai DARTH 99\n",
    "                          \n",
    "            # make y axis log:\n",
    "            ax.set_yscale(\"log\")     \n",
    "\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            if noise_percentage == \"0\":\n",
    "                fig.savefig(f\"../../experiments/plots/histogram_{ds_name}_k{k}_rt{r_target}.pdf\", format=\"pdf\", bbox_inches = 'tight')\n",
    "                print(f\"Saved plot at ./../../experiments/plots/histogram_{ds_name}_k{k}_rt{r_target}.pdf\")\n",
    "            else:\n",
    "                fig.savefig(f\"../../experiments/plots/histogram_noise{noise_percentage}_{ds_name}_k{k}_rt{r_target}.pdf\", format=\"pdf\", bbox_inches = 'tight')\n",
    "                print(f\"Saved plot at ./../../experiments/plots/histogram_noise{noise_percentage}_{ds_name}_k{k}_rt{r_target}.pdf\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"DEEP100M\"]#[\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [\"50\"]#[\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"1.00\"]# \"0.95\"]#, \"0.99\"]\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "bar_width = 0.2\n",
    "s = 1000\n",
    "\n",
    "\n",
    "for i, ds_name in enumerate(all_datasets):\n",
    "    M, efS, efC = dataset_params[ds_name][\"M\"], dataset_params[ds_name][\"efS\"], dataset_params[ds_name][\"efC\"]\n",
    "   \n",
    "    for r_target in all_r_targets:\n",
    "                \n",
    "        for k in all_k_values:\n",
    "            ipi, mpi = 4011, 802#interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"ipi\"], interval_conf[ds_name][k][f\"{float(r_target):.2f}\"][\"mpi\"]\n",
    "            f_r_target = 1.00#float(r_target)\n",
    "                    \n",
    "            no_early_stop_df = pd.read_csv(f\"../../experiments/results/no-early-stop/testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\", \"RDE\", \"TDR\", \"NRS\"])\n",
    "            print(len(no_early_stop_df))\n",
    "            no_early_stop_df = no_early_stop_df[no_early_stop_df[\"r\"] >= f_r_target]              \n",
    "            print(len(no_early_stop_df))\n",
    "            \n",
    "            darth_df = pd.read_csv(f\"../../experiments/results/early-stop-testing/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{f_r_target}_ipi{ipi}_mpi{mpi}.txt\", usecols=[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\", \"RDE\", \"TDR\", \"NRS\"])        \n",
    "  \n",
    "            darth_df = darth_df[darth_df[\"qid\"].isin(no_early_stop_df[\"qid\"])]\n",
    "                    \n",
    "            recall_darth = darth_df[\"r_actual\"].mean()\n",
    "            \n",
    "            speedup_vs_no_early_stop = no_early_stop_df[\"elaps_ms\"].mean() / darth_df[\"elaps_ms\"].mean()\n",
    "            print(f\"Speedup vs no early stop: {speedup_vs_no_early_stop}\")\n",
    "            \n",
    "            darth_rqt = darth_df[darth_df[\"r_actual\"] < f_r_target].shape[0]\n",
    "            hnsw_rqt = hnsw_df[hnsw_df[\"r\"] < f_r_target].shape[0]\n",
    "            \n",
    "            print(f\"Queries DARTH: {darth_rqt}\")\n",
    "\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(9,5))\n",
    "            #ax.hist(hnsw_df[\"r\"], alpha=0.9, label=\"REM\", color=algorithm_conf[\"HNSW\"][\"color\"], linewidth=2, edgecolor=\"black\", bins=20)\n",
    "            ax.hist(darth_df[\"r_actual\"], alpha=0.9, label=\"DARTH\", color=algorithm_conf[\"DARTH\"][\"color\"], linewidth=2, edgecolor=\"black\", bins=7)\n",
    "            ax.axvline(x=f_r_target, color=\"red\", linestyle=\"--\", label=f\"Rt={f_r_target}\", linewidth=3)\n",
    "            ax.legend(fontsize=28)\n",
    "            ax.set_xlabel(\"Recall\")\n",
    "            ax.set_ylabel(\"# Queries\")\n",
    "            ax.grid(alpha=0.7, linestyle=\"--\")     \n",
    "        \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "            \n",
    "            fig.tight_layout()\n",
    "            fig.savefig(f\"../../experiments/plots/histogram_{ds_name}_k{k}_rt{r_target}.pdf\", format=\"pdf\", bbox_inches = 'tight')\n",
    "            print(f\"Saved plot at ./../../experiments/plots/histogram_{ds_name}_k{k}_rt{r_target}.pdf\")\n",
    "           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localPyLibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
