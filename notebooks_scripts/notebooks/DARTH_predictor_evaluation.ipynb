{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor Validation and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.linear_model import LinearRegression  # type: ignore\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import os\n",
    "\n",
    "def get_dataset_name(M, efC, efS, query_num, ds_name, k, logint):\n",
    "    return f\"/data/mchatzakis/et_training_data/early-stop-training/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{query_num}_li{logint}.txt\"\n",
    "\n",
    "def get_validation_dataset_name(M, efC, efS, query_num, ds_name, k, logint): \n",
    "    return f\"../../experiments/results/validation_logging/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{query_num}_li{logint}.txt\"\n",
    "\n",
    "def get_model_name(M, ef, s, ds_name, k, logint, selected_features, n_estimators=100):\n",
    "    return f\"../../predictor_models/lightgbm/{ds_name}_M{M}_ef{ef}_s{s}_k{k}_nestim{n_estimators}_li{logint}_{selected_features}.txt\"\n",
    "\n",
    "def compute_P99(y_true, y_pred):\n",
    "    y_diff = np.abs(y_true - y_pred)\n",
    "    return np.percentile(y_diff, 99)\n",
    "\n",
    "def compute_P1(y_true, y_pred):\n",
    "    y_diff = np.abs(y_true - y_pred)\n",
    "    return np.percentile(y_diff, 1)\n",
    "\n",
    "def get_interval_tuning_dataset_name(M, efC, efS, s, ds_name, k, r_target, ipi, mpi, mode=\"search\"):\n",
    "    return f\"../../experiments/results/interval-tuning/{mode}/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}_tr{r_target:.2f}_ipi{ipi}_mpi{mpi}.txt\"\n",
    "\n",
    "def get_no_early_stop_dataset_name(M, efC, efS, s, ds_name, k):\n",
    "    return f\"../../experiments/results/no-early-stop/validation/{ds_name}/k{k}/M{M}_efC{efC}_efS{efS}_qs{s}.txt\"\n",
    "\n",
    "dataset_params = {\n",
    "        \"SIFT100M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"li\": 1,\n",
    "            \"label\": \"SIFT100M\",\n",
    "        },\n",
    "        \"GIST1M\": {\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 1000,\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GIST1M\",\n",
    "        },\n",
    "        \"GLOVE100\": {\n",
    "            \"M\": 16,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 500,\n",
    "            \"li\": 1,\n",
    "            \"label\": \"GLOVE1M\",\n",
    "        },\n",
    "        \"DEEP100M\":{\n",
    "            \"M\": 32,\n",
    "            \"efC\": 500,\n",
    "            \"efS\": 750,\n",
    "            \"li\": 1,\n",
    "            \"label\": \"DEEP100M\",\n",
    "        },\n",
    "        \"T2I100M\":{\n",
    "            \"M\": 80,\n",
    "            \"efC\": 1000,\n",
    "            \"efS\": 2500,\n",
    "            \"li\": 2,\n",
    "            \"label\": \"T2I100M\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "index_metric_feats = [\"step\", \"dists\", \"inserts\"]\n",
    "neighbor_distances_feats = [\"first_nn_dist\", \"nn_dist\", \"furthest_dist\"]\n",
    "neighbor_stats_feats = [\"avg_dist\", \"variance\", \"percentile_25\", \"percentile_50\", \"percentile_75\"]\n",
    "all_feats = index_metric_feats + neighbor_distances_feats + neighbor_stats_feats\n",
    "\n",
    "columns_to_load = [\"qid\", \"elaps_ms\"] + all_feats + [\"r\", \"feats_collect_time_ms\"]\n",
    "    \n",
    "model_conf = {\n",
    "    \"lightgbm\": lgb.LGBMRegressor(objective='regression', random_state=SEED, n_estimators=n_estimators, verbose = -1),\n",
    "}\n",
    "\n",
    "feature_classes = {\n",
    "    \"index_metric_feats\": index_metric_feats,\n",
    "    \"neighbor_distances_feats\": neighbor_distances_feats,\n",
    "    \"neighbor_stats_feats\": neighbor_stats_feats,\n",
    "    \"index_metrics_and_neighbor_distances\": index_metric_feats + neighbor_distances_feats,\n",
    "    \"index_metrics_and_neighbor_stats\": index_metric_feats + neighbor_stats_feats,\n",
    "    \"neighbor_distances_and_neighbor_stats\": neighbor_distances_feats + neighbor_stats_feats,\n",
    "    \"all_feats\": all_feats,\n",
    "}\n",
    "\n",
    "print(columns_to_load)\n",
    "\n",
    "PLOTS_DIR = \"./../../experiments/revision-plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Termination Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name = \"SIFT100M\"\n",
    "M = dataset_params[ds_name][\"M\"]\n",
    "efC = dataset_params[ds_name][\"efC\"]\n",
    "efS = dataset_params[ds_name][\"efS\"]\n",
    "k=100\n",
    "li=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_data_all = dd.read_csv(get_dataset_name(M, efC, efS, 10000, ds_name, k, li), usecols=columns_to_load)\n",
    "data_all = dask_data_all.compute()\n",
    "print(data_all.shape)\n",
    "\n",
    "total_queries = 2000\n",
    "metric = \"elaps_ms\"\n",
    "\n",
    "queries = [] # List of [time, recall]\n",
    "for qid in range(total_queries):\n",
    "    curr_query = data_all[data_all[\"qid\"] == qid]\n",
    "    recalls, times = curr_query[\"r\"], curr_query[metric]\n",
    "    queries.append([times, recalls])\n",
    "\n",
    "# Define summaries for each query\n",
    "summarized_queries = []\n",
    "for times, recalls in queries:\n",
    "    # Compute mean of times and recalls\n",
    "    mean_time = np.mean(times)\n",
    "    mean_recall = np.mean(recalls)\n",
    "    summarized_queries.append([mean_time, mean_recall])\n",
    "\n",
    "# Convert summarized queries to a NumPy array\n",
    "summarized_queries_array = np.array(summarized_queries)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "normalized_queries = scaler.fit_transform(summarized_queries_array)\n",
    "\n",
    "# Clustering with 20 clusters\n",
    "num_queries_to_plot = 4\n",
    "kmeans = KMeans(n_clusters=num_queries_to_plot, random_state=SEED)\n",
    "labels = kmeans.fit_predict(normalized_queries)\n",
    "\n",
    "# Select one query from each cluster\n",
    "selected_queries = []\n",
    "for cluster_id in range(num_queries_to_plot):\n",
    "    cluster_queries = [queries[i] for i in range(len(queries)) if labels[i] == cluster_id]\n",
    "    selected_queries.append(cluster_queries[0])  # Choose the first query in each cluster\n",
    "\n",
    "# save selected queries\n",
    "selected_queries_dict = {}\n",
    "for i, query in enumerate(selected_queries):\n",
    "    times, recalls = query\n",
    "    selected_queries_dict[i] = {\"times\": times.tolist(), \"recalls\": recalls.tolist()}\n",
    "\n",
    "print(selected_queries_dict)\n",
    "\n",
    "with open(f\"../../experiments/generated_json/selected_queries.json\", \"w\") as f:\n",
    "    json.dump(selected_queries_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "selected_queries = []\n",
    "selected_queries_dict = {}\n",
    "with open(f\"../../experiments/generated_json/selected_queries.json\", \"r\") as f:\n",
    "    selected_queries_dict = json.load(f)\n",
    "\n",
    "for i in range(len(selected_queries_dict)):\n",
    "    query = selected_queries_dict[str(i)]\n",
    "    times, recalls = query[\"times\"], query[\"recalls\"]\n",
    "    selected_queries.append([times, recalls])\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "\n",
    "# Plot the selected queries\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "markers = [\"o\", \"s\", \"D\", \"v\", \"^\", \">\", \"<\", \"p\", \"P\", \"*\"]\n",
    "\n",
    "# Determine the x-axis limits\n",
    "max_time = max([max(query[0]) / 1000 for query in selected_queries])\n",
    "\n",
    "# Fill the background above y=0.8 with light gray\n",
    "ax.fill_between(\n",
    "    [-1, 15],  # x-values covering the whole range\n",
    "    0.8, 1.1,  # y-range for the fill\n",
    "    color='lightgray', alpha=0.5  # fill color and transparency\n",
    ")\n",
    "\n",
    "plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.8, linewidth=2)\n",
    "\n",
    "query_num = 0\n",
    "for i, query in enumerate(selected_queries):\n",
    "    \n",
    "    times, recalls = query\n",
    "    times = list(times)\n",
    "    recalls = list(recalls)\n",
    "    times = [t / 1000 for t in times]\n",
    "\n",
    "    if colors[i] == \"green\":\n",
    "        continue\n",
    "\n",
    "    # Find the index where recall reaches 0.8\n",
    "    index = -1\n",
    "    for j, r in enumerate(recalls):\n",
    "        if r >= 0.80:\n",
    "            plt.axvline(\n",
    "                x=times[j], color=colors[i], linestyle=\":\", ymax=r - 0.06, linewidth=2, alpha=0.8, ymin=0.045\n",
    "            )\n",
    "            index = j\n",
    "            break\n",
    "\n",
    "    # Downsample the data\n",
    "    downsample_factor = 1200\n",
    "    downsampled_times = []\n",
    "    downsampled_recalls = []\n",
    "    recalls_to_put_markers = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "    already_found_recalls = [False] * len(recalls_to_put_markers)\n",
    "    marker_plot_times = []\n",
    "    marker_plot_recalls = []\n",
    "\n",
    "    for j, r in enumerate(recalls):\n",
    "        if r in recalls_to_put_markers and not already_found_recalls[recalls_to_put_markers.index(r)]:\n",
    "            marker_plot_times.append(times[j])\n",
    "            marker_plot_recalls.append(r)\n",
    "            already_found_recalls[recalls_to_put_markers.index(r)] = True\n",
    "            downsampled_times.append(times[j])\n",
    "            downsampled_recalls.append(r)\n",
    "        if j == len(recalls) - 1:\n",
    "            marker_plot_times.append(times[j])\n",
    "            marker_plot_recalls.append(r)\n",
    "\n",
    "    if downsampled_times and downsampled_times[0] != 0:\n",
    "        downsampled_times.append(times[-1])\n",
    "        downsampled_recalls.append(recalls[-1])\n",
    "        downsampled_times.insert(0, 0)\n",
    "        downsampled_recalls.insert(0, 0)\n",
    "        marker_plot_times.append(0)\n",
    "        marker_plot_recalls.append(0)\n",
    "\n",
    "    # Split the data into before and after recall 0.8\n",
    "    before_08_times = [t for t, r in zip(downsampled_times, downsampled_recalls) if r <= 0.8]\n",
    "    before_08_recalls = [r for r in downsampled_recalls if r <= 0.8]\n",
    "    after_08_times = [t for t, r in zip(downsampled_times, downsampled_recalls) if r >= 0.8]\n",
    "    after_08_recalls = [r for r in downsampled_recalls if r >= 0.8]\n",
    "\n",
    "    # Plot the lines with different transparency\n",
    "    query_num += 1\n",
    "    ax.plot(before_08_times, before_08_recalls, linestyle=\"-\", alpha=0.9, color=colors[i], linewidth=3)\n",
    "    ax.plot(after_08_times, after_08_recalls, linestyle=\"--\", alpha=0.7, color=colors[i], linewidth=3)\n",
    "\n",
    "    # Split the markers\n",
    "    marker_before_08_times = [t for t, r in zip(marker_plot_times, marker_plot_recalls) if r <= 0.8]\n",
    "    marker_before_08_recalls = [r for r in marker_plot_recalls if r <= 0.8]\n",
    "    marker_after_08_times = [t for t, r in zip(marker_plot_times, marker_plot_recalls) if r >= 0.8]\n",
    "    marker_after_08_recalls = [r for r in marker_plot_recalls if r >= 0.8]\n",
    "\n",
    "    # Plot the scatter points for recall markers\n",
    "    ax.scatter(marker_before_08_times, marker_before_08_recalls, color=colors[i], s=300, zorder=5, alpha=0.9, marker=markers[i])\n",
    "    ax.scatter(marker_after_08_times, marker_after_08_recalls, color=colors[i], s=300, alpha=0.4, marker=markers[i])\n",
    "\n",
    "    # Add legend for each line including the marker:\n",
    "    ax.plot([], [], '-', label=f\"Query {query_num}\", color=colors[i], linewidth=3, marker=markers[i], markersize=20)\n",
    "\n",
    "#lines_for_legend = [line for line in ax.get_lines() if not line.get_label().startswith('_')]\n",
    "\n",
    "# Add custom legend for the \"Wasted Effort\" area\n",
    "#custom_legend = [plt.Line2D([0], [0], color='lightgray', lw=10, alpha=0.5)]\n",
    "#ax.legend(custom_legend + lines_for_legend, ['Wasted Effort'] + [line.get_label() for line in lines_for_legend], loc=\"lower right\")\n",
    "ax.text(max_time * 1.02, 0.85, 'Wasted\\n  Effort', fontsize=28, color='black', bbox=dict(facecolor='lightgray', alpha=0.0))\n",
    "\n",
    "ax.set_xlabel(\"Search Time (ms)\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.legend(loc=\"lower right\") \n",
    "\n",
    "#lock x axis to 0-14\n",
    "ax.set_xlim(-0.5, 14)\n",
    "ax.set_ylim(-0.08, 1.1)\n",
    "\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save as PDF\n",
    "fig.savefig(f\"./{PLOTS_DIR}/early_termination_opportunities_test3.pdf\", bbox_inches='tight')\n",
    "print(f\"Fig saved at ./{PLOTS_DIR}/early_termination_opportunities_test3.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "selected_queries = []\n",
    "selected_queries_dict = {}\n",
    "with open(f\"../../experiments/generated_json/selected_queries.json\", \"r\") as f:\n",
    "    selected_queries_dict = json.load(f)\n",
    "\n",
    "for i in range(len(selected_queries_dict)):\n",
    "    query = selected_queries_dict[str(i)]\n",
    "    times, recalls = query[\"times\"], query[\"recalls\"]\n",
    "    selected_queries.append([times, recalls])\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 30})\n",
    "\n",
    "# Plot the selected queries\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\"]\n",
    "markers = [\"o\", \"s\", \"D\", \"v\", \"^\", \">\", \"<\", \"p\", \"P\", \"*\"]\n",
    "\n",
    "# Determine the x-axis limits\n",
    "max_time = max([max(query[0]) / 1000 for query in selected_queries])\n",
    "\n",
    "# Fill the background above y=0.8 with light gray\n",
    "ax.fill_between(\n",
    "    [-1, 15],  # x-values covering the whole range\n",
    "    0.8, 1.1,  # y-range for the fill\n",
    "    color='lightgray', alpha=0.5  # fill color and transparency\n",
    ")\n",
    "\n",
    "plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.8, linewidth=2)\n",
    "\n",
    "query_num = 0\n",
    "for i, query in enumerate(selected_queries):\n",
    "    \n",
    "    #if (i != 0 and i != 1):\n",
    "    #    continue\n",
    "    \n",
    "    times, recalls = query\n",
    "    times = list(times)\n",
    "    recalls = list(recalls)\n",
    "    times = [t / 1000 for t in times]\n",
    "\n",
    "    if colors[i] == \"green\":\n",
    "        continue\n",
    "\n",
    "    # Find the index where recall reaches 0.8\n",
    "    index = -1\n",
    "    for j, r in enumerate(recalls):\n",
    "        if r >= 0.80:\n",
    "            plt.axvline(\n",
    "                x=times[j], color=colors[i], linestyle=\":\", ymax=r - 0.06, linewidth=2, alpha=0.8, ymin=0.045\n",
    "            )\n",
    "            index = j\n",
    "            break\n",
    "\n",
    "    # Downsample the data\n",
    "    downsample_factor = 1200\n",
    "    downsampled_times = []\n",
    "    downsampled_recalls = []\n",
    "    recalls_to_put_markers = [0.2, 0.4, 0.6, 0.8, 1]\n",
    "    already_found_recalls = [False] * len(recalls_to_put_markers)\n",
    "    marker_plot_times = []\n",
    "    marker_plot_recalls = []\n",
    "\n",
    "    for j, r in enumerate(recalls):\n",
    "        if r in recalls_to_put_markers and not already_found_recalls[recalls_to_put_markers.index(r)]:\n",
    "            marker_plot_times.append(times[j])\n",
    "            marker_plot_recalls.append(r)\n",
    "            already_found_recalls[recalls_to_put_markers.index(r)] = True\n",
    "            downsampled_times.append(times[j])\n",
    "            downsampled_recalls.append(r)\n",
    "        if j == len(recalls) - 1:\n",
    "            marker_plot_times.append(times[j])\n",
    "            marker_plot_recalls.append(r)\n",
    "\n",
    "    if downsampled_times and downsampled_times[0] != 0:\n",
    "        downsampled_times.append(times[-1])\n",
    "        downsampled_recalls.append(recalls[-1])\n",
    "        downsampled_times.insert(0, 0)\n",
    "        downsampled_recalls.insert(0, 0)\n",
    "        marker_plot_times.append(0)\n",
    "        marker_plot_recalls.append(0)\n",
    "\n",
    "    # Split the data into before and after recall 0.8\n",
    "    before_08_times = [t for t, r in zip(downsampled_times, downsampled_recalls) if r <= 0.8]\n",
    "    before_08_recalls = [r for r in downsampled_recalls if r <= 0.8]\n",
    "    after_08_times = [t for t, r in zip(downsampled_times, downsampled_recalls) if r >= 0.8]\n",
    "    after_08_recalls = [r for r in downsampled_recalls if r >= 0.8]\n",
    "\n",
    "    # Plot the lines with different transparency\n",
    "    query_num += 1\n",
    "    ax.plot(before_08_times, before_08_recalls, linestyle=\"-\", alpha=0.9, color=colors[i], linewidth=3)\n",
    "    ax.plot(after_08_times, after_08_recalls, linestyle=\"--\", alpha=0.7, color=colors[i], linewidth=3)\n",
    "\n",
    "    # Split the markers\n",
    "    marker_before_08_times = [t for t, r in zip(marker_plot_times, marker_plot_recalls) if r <= 0.8]\n",
    "    marker_before_08_recalls = [r for r in marker_plot_recalls if r <= 0.8]\n",
    "    marker_after_08_times = [t for t, r in zip(marker_plot_times, marker_plot_recalls) if r >= 0.8]\n",
    "    marker_after_08_recalls = [r for r in marker_plot_recalls if r >= 0.8]\n",
    "\n",
    "    # Plot the scatter points for recall markers\n",
    "    ax.scatter(marker_before_08_times, marker_before_08_recalls, color=colors[i], s=300, zorder=5, alpha=0.9, marker=markers[i])\n",
    "    ax.scatter(marker_after_08_times, marker_after_08_recalls, color=colors[i], s=300, alpha=0.4, marker=markers[i])\n",
    "\n",
    "    # Add legend for each line including the marker:\n",
    "    ax.plot([], [], '-', label=f\"Query {query_num}\", color=colors[i], linewidth=3, marker=markers[i], markersize=20)\n",
    "\n",
    "#lines_for_legend = [line for line in ax.get_lines() if not line.get_label().startswith('_')]\n",
    "\n",
    "# Add custom legend for the \"Wasted Effort\" area\n",
    "#custom_legend = [plt.Line2D([0], [0], color='lightgray', lw=10, alpha=0.5)]\n",
    "#ax.legend(custom_legend + lines_for_legend, ['Wasted Effort'] + [line.get_label() for line in lines_for_legend], loc=\"lower right\")\n",
    "#ax.text(max_time * 1.02, 0.85, 'Wasted\\n  Effort', fontsize=28, color='black', bbox=dict(facecolor='lightgray', alpha=0.0))\n",
    "\n",
    "ax.set_xlabel(\"Search Time (ms)\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "ax.legend(loc=\"lower right\") \n",
    "\n",
    "#lock x axis to 0-14\n",
    "ax.set_xlim(-0.5, 14)\n",
    "ax.set_ylim(-0.08, 1.1)\n",
    "\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save as PDF\n",
    "fig.savefig(f\"./{PLOTS_DIR}/early_termination_opportunities_presentation_3.png\")#, bbox_inches='tight')\n",
    "print(f\"Fig saved at ./{PLOTS_DIR}/early_termination_opportunities_presentation_3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "with open( \"../../experiments/generated_json/predictor_validation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "all_training_queries = [\"10000\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_datasets = [\"GLOVE100\", \"SIFT100M\", \"GIST1M\", \"DEEP100M\"]#, \"T2I100M\"]\n",
    "#all_lis = [\"1\"]\n",
    "all_metrics = [\"r2\", \"mse\", \"mae\"]\n",
    "all_feature_types = feature_classes.keys()\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "# For every feature class, create a list fo the metrics for each conf\n",
    "#feature_scores = {}\n",
    "#for feature_class in all_feature_types:\n",
    "#    for li in all_lis:\n",
    "#        feature_scores[f\"{feature_class}_{li}\"] = {}\n",
    "#        for metric in all_metrics:\n",
    "#            feature_scores[f\"{feature_class}_{li}\"][metric] = []\n",
    "\n",
    "feature_scores = {}\n",
    "for feature_class in all_feature_types:\n",
    "    feature_scores[f\"{feature_class}\"] = {}\n",
    "    for metric in all_metrics:\n",
    "        feature_scores[f\"{feature_class}\"][metric] = []\n",
    "\n",
    "\n",
    "#print(feature_scores)\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    best_models[ds_name] = {}\n",
    "        \n",
    "    for i, k in enumerate(all_k_values):        \n",
    "        best_r2 = -1\n",
    "        best_queries = -1\n",
    "        best_li = -1\n",
    "        best_p99 = -1\n",
    "        best_mse = 1000000\n",
    "        best_features = \"\"\n",
    "        \n",
    "        for j, metric in enumerate(all_metrics):\n",
    "            #for li in all_lis:\n",
    "            for selected_features in all_feature_types:#feature_classes.keys():\n",
    "                    \n",
    "                metric_values = [results[ds_name][k][s][li][selected_features][metric] for s in all_training_queries]                    \n",
    "                queries, metrics = all_training_queries, metric_values\n",
    "                    \n",
    "                feature_scores[f\"{selected_features}_{li}\"][metric].append(metrics)\n",
    "\n",
    "#print(feature_scores)\n",
    "\n",
    "# Average the metrics for each feature class\n",
    "feature_avg_scores = {}\n",
    "for feature_class in all_feature_types:\n",
    "    for li in all_lis:\n",
    "        feature_avg_scores[f\"{feature_class}_{li}\"] = {}\n",
    "        for metric in all_metrics:\n",
    "            feature_avg_scores[f\"{feature_class}_{li}\"][metric] = np.mean(feature_scores[f\"{feature_class}_{li}\"][metric])\n",
    "\n",
    "# Sort by min mse\n",
    "sorted_feature_avg_scores = {k: v for k, v in sorted(feature_avg_scores.items(), key=lambda item: item[1][\"mse\"], reverse=False)}\n",
    "sorted_feature_avg_scores\n",
    "\n",
    "# Print the first 5\n",
    "for i, (feature_class, metrics) in enumerate(sorted_feature_avg_scores.items()):\n",
    "    #if i >= 5:\n",
    "    #    break\n",
    "\n",
    "    print(f\"{feature_class}\")\n",
    "    for metric in all_metrics:\n",
    "        if metric == \"p99\" or metric == \"r2\" or metric == \"\" or metric == \"mae\":\n",
    "            print(f\"    {metric}: {metrics[metric]:.2f}\")\n",
    "        else:\n",
    "            print(f\"    {metric}: {metrics[metric]:.3f}\")\n",
    "\n",
    "# Pick the best feature class and print it\n",
    "best_feature_class = list(sorted_feature_avg_scores.keys())[0]\n",
    "best_feature_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "with open( \"../../experiments/generated_json/predictor_validation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "all_training_queries = [\"10000\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_datasets = [\"GLOVE100\", \"SIFT100M\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_metrics = [\"r2\", \"mse\", \"mae\"]\n",
    "all_feature_types = feature_classes.keys()\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "feature_scores = {}\n",
    "for feature_class in all_feature_types:\n",
    "    feature_scores[f\"{feature_class}\"] = {}\n",
    "    for metric in all_metrics:\n",
    "        feature_scores[f\"{feature_class}\"][metric] = []\n",
    "\n",
    "\n",
    "#print(feature_scores)\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    best_models[ds_name] = {}\n",
    "        \n",
    "    for i, k in enumerate(all_k_values):        \n",
    "        best_r2 = -1\n",
    "        best_queries = -1\n",
    "        best_li = -1\n",
    "        best_p99 = -1\n",
    "        best_mse = 1000000\n",
    "        best_features = \"\"\n",
    "        \n",
    "        for j, metric in enumerate(all_metrics):\n",
    "            for selected_features in all_feature_types:\n",
    "                li = str(dataset_params[ds_name][\"li\"])\n",
    "                metric_values = [results[ds_name][k][s][li][selected_features][metric] for s in all_training_queries]                    \n",
    "                queries, metrics = all_training_queries, metric_values\n",
    "                    \n",
    "                feature_scores[f\"{selected_features}\"][metric].append(metrics)\n",
    "\n",
    "#print(feature_scores)\n",
    "\n",
    "# Average the metrics for each feature class\n",
    "feature_avg_scores = {}\n",
    "for feature_class in all_feature_types:\n",
    "    feature_avg_scores[f\"{feature_class}\"] = {}\n",
    "    for metric in all_metrics:\n",
    "        feature_avg_scores[f\"{feature_class}\"][metric] = np.mean(feature_scores[f\"{feature_class}\"][metric])\n",
    "\n",
    "# Sort by min mse\n",
    "sorted_feature_avg_scores = {k: v for k, v in sorted(feature_avg_scores.items(), key=lambda item: item[1][\"mse\"], reverse=False)}\n",
    "sorted_feature_avg_scores\n",
    "\n",
    "# Print the first 5\n",
    "for i, (feature_class, metrics) in enumerate(sorted_feature_avg_scores.items()):\n",
    "    #if i >= 5:\n",
    "    #    break\n",
    "\n",
    "    print(f\"{feature_class}\")\n",
    "    for metric in all_metrics:\n",
    "        if metric == \"mse\" or metric == \"mae\":\n",
    "            print(f\"    {metric}: {metrics[metric]:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {metric}: {metrics[metric]:.2f}\")\n",
    "\n",
    "# Pick the best feature class and print it\n",
    "best_feature_class = list(sorted_feature_avg_scores.keys())[0]\n",
    "best_feature_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"GLOVE100\", \"SIFT100M\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "best_feature_class = \"all_feats\"\n",
    "#best_li = 1\n",
    "best_s = 10000\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    training_results_dict = {}\n",
    "    with open(f\"../../experiments/generated_json/training_results_{ds_name}.json\", \"r\") as f:\n",
    "        training_results_dict = json.load(f)\n",
    "    \n",
    "    dataset_samples = 0\n",
    "    dataset_train_time = 0\n",
    "    \n",
    "    for k in all_k_values:\n",
    "        best_li = str(dataset_params[ds_name][\"li\"])\n",
    "        training_time = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"training_time\"]\n",
    "        training_samples = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"training_data_size\"]\n",
    "        learning_rate = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"learning_rate\"]\n",
    "        feature_importances = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"feature_importances\"]\n",
    "        \n",
    "        dataset_samples += training_samples\n",
    "        dataset_train_time += training_time\n",
    "    \n",
    "    dataset_samples = dataset_samples / len(training_results_dict.keys())\n",
    "    dataset_train_time = dataset_train_time / len(training_results_dict.keys())\n",
    "    \n",
    "    print(f\"Dataset: {ds_name} => Samples: {dataset_samples:.0f}, Training Time: {dataset_train_time:.0f}s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature_class = \"all_feats\"\n",
    "best_s = 10000\n",
    "all_datasets = [\"GLOVE100\", \"SIFT100M\", \"GIST1M\", \"DEEP100M\"]\n",
    "\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "\n",
    "feature_scores = {}\n",
    "for ds_name in all_datasets:\n",
    "    training_results_dict = {}\n",
    "    with open(f\"../../experiments/generated_json/training_results_{ds_name}.json\", \"r\") as f:\n",
    "        training_results_dict = json.load(f)\n",
    "    \n",
    "    for k in training_results_dict.keys():\n",
    "        #print(training_results_dict[ds_name][k][str(best_s)][str(best_li)][best_feature_class])\n",
    "        best_li = str(dataset_params[ds_name][\"li\"])\n",
    "        training_time = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"training_time\"]\n",
    "        training_samples = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"training_data_size\"]\n",
    "        learning_rate = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"learning_rate\"]\n",
    "        feature_importances = training_results_dict[k][str(best_s)][str(best_li)][best_feature_class][\"feature_importances\"]\n",
    "        for entry in feature_importances:\n",
    "            feature_name = entry[\"Feature\"]\n",
    "            importance_score = entry[\"Importance\"]\n",
    "            \n",
    "            if feature_name not in feature_scores:\n",
    "                feature_scores[feature_name] = [importance_score]\n",
    "            else:\n",
    "                feature_scores[feature_name].append(importance_score)\n",
    "\n",
    "feature_avg_scores = {}\n",
    "for feature_name, scores in feature_scores.items():\n",
    "    feature_avg_scores[feature_name] = np.mean(scores)\n",
    "\n",
    "sorted_feature_avg_scores = {k: v for k, v in sorted(feature_avg_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "#for i, (feature_name, score) in enumerate(sorted_feature_avg_scores.items()):\n",
    "#    print(f\">> {feature_name}: {score:.2f}\")\n",
    "    \n",
    "# Transform to perc\n",
    "total_score = sum(feature_avg_scores.values())\n",
    "perc_feature_avg_scores = {k: v / total_score for k, v in feature_avg_scores.items()}\n",
    "sorted_perc_feature_avg_scores = {k: v for k, v in sorted(perc_feature_avg_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "# Print again:\n",
    "for i, (feature_name, score) in enumerate(sorted_perc_feature_avg_scores.items()):\n",
    "    print(f\">> {feature_name}: {(score * 100):.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations for the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 42})\n",
    "\n",
    "# Mapping for metric axis titles\n",
    "metric_axis_titles = {\n",
    "    \"mae\": r\"$MAE$\",\n",
    "    \"mse\": r\"$MSE$\",\n",
    "    \"r2\": r\"$R^2$\",\n",
    "    \"p99\": r\"$P99$\"\n",
    "}\n",
    "\n",
    "all_training_queries = [\"100\", \"1000\", \"5000\", \"10000\", \"15000\", \"20000\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_metrics = [\"mse\"]#[\"r2\", \"mse\", \"mae\"]\n",
    "\n",
    "#universal_best_li = 1\n",
    "universal_best_model_features = \"all_feats\"\n",
    "\n",
    "# Create a dummy plot for the legend\n",
    "fig, ax = plt.subplots(figsize=(8, 1))  # Adjust width for a horizontal layout\n",
    "\n",
    "# Add dummy lines for the legend\n",
    "for k in all_k_values:\n",
    "    ax.plot([], [], label=f\"k={k}\", marker=\"o\", alpha=1, markersize=30, linewidth=4)\n",
    "\n",
    "legend = ax.legend(\n",
    "    loc='center', \n",
    "    ncol=len(all_k_values), \n",
    "    frameon=False,\n",
    "    fontsize=48,\n",
    "    handletextpad=0.3,  # Adjust padding between handle and text\n",
    "    columnspacing=0.5,  # Adjust space between columns\n",
    "    labelspacing=0.3    # Adjust vertical space between labels\n",
    ")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# Save the legend figure\n",
    "legend_fig_path = f\"{PLOTS_DIR}training_queries_legend_only.pdf\"\n",
    "fig.savefig(legend_fig_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved {legend_fig_path}\")\n",
    "plt.show()\n",
    "plt.close(fig)  # Close the figure\n",
    "\n",
    "# Iterate over datasets and metrics to generate and save figures\n",
    "for ds_name in all_datasets:\n",
    "    for metric in all_metrics:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        for k in all_k_values:\n",
    "            best_li = str(dataset_params[ds_name][\"li\"])\n",
    "            metric_values = [\n",
    "                results[ds_name][k][s][str(best_li)][str(universal_best_model_features)][metric] \n",
    "                for s in all_training_queries\n",
    "            ]\n",
    "            \n",
    "            if metric == \"mse\" or metric == \"mae\":\n",
    "                metric_values = [v * 100 for v in metric_values]\n",
    "                ax.set_ylabel(f\" {metric_axis_titles[metric]} {r'$(10^{2})$'}\")\n",
    "            else:\n",
    "                ax.set_ylabel(metric_axis_titles[metric])\n",
    "                \n",
    "            ax.plot(\n",
    "                [int(q) for q in all_training_queries],\n",
    "                metric_values,\n",
    "                label=f\"k={k}\",\n",
    "                marker=\"o\",\n",
    "                alpha=0.8,\n",
    "                linewidth=3,\n",
    "                markersize=23\n",
    "            )\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "        \n",
    "\n",
    "        ax.set_xlabel(\"# Training Queries\")\n",
    "        ax.grid(alpha=0.8, linestyle=\"--\", linewidth=1)\n",
    "        #ax.tick_params(axis='x', labelsize=44)\n",
    "        output_path = f\"{PLOTS_DIR}training_queries_{ds_name}_{metric}.pdf\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(output_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "        if metric == \"mse\":\n",
    "            print(f\"Saved {output_path}\")\n",
    "            plt.show()\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristic Parameter Generation for DARTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reccomend the parameters for the early termination naive baseline:\n",
    "all_datasets =  [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    data_stat_filename = f\"../../experiments/generated_json/data_stats_{ds_name}.json\"\n",
    "    data_stats = {}\n",
    "    with open(data_stat_filename, \"r\") as f:\n",
    "        data_stats = json.load(f)\n",
    "    \n",
    "    for k in all_k_values:\n",
    "        \n",
    "        conf_str = f\"{ds_name}_tuples_k{k}=(\\n\"\n",
    "        for r_target in all_r_targets:\n",
    "            avg_dists = data_stats[k][\"distance_calcs_to_reach_target_recall\"][r_target][\"avg\"]\n",
    "            reccomended_stop_point = avg_dists\n",
    "            conf_str += f\"  \\\"{float(r_target):.2f} {reccomended_stop_point:.0f}\\\" \\n\"\n",
    "        conf_str += \")\"\n",
    "        print(conf_str, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations for DARTH Heuristic methods (evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [\"50\"]\n",
    "all_r_targets = [\"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "ipi_div_factors = [2, 4]\n",
    "mpi_div_factors = [4, 8, 10]\n",
    "\n",
    "heuristic_recommendations_params = {}\n",
    "for ipi_div in ipi_div_factors:\n",
    "    for mpi_div in mpi_div_factors:\n",
    "        if ipi_div > mpi_div:\n",
    "            continue\n",
    "        \n",
    "        heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"] = {}\n",
    "        for ds_name in all_datasets:\n",
    "            heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name] = {}\n",
    "            data_stat_filename = f\"../../experiments/generated_json/data_stats_{ds_name}.json\"\n",
    "            data_stats = {}\n",
    "            with open(data_stat_filename, \"r\") as f:\n",
    "                data_stats = json.load(f)\n",
    "            \n",
    "            for k in all_k_values:\n",
    "                heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][k] = {}\n",
    "                for r_target in all_r_targets:\n",
    "                    avg_dists = data_stats[k][\"distance_calcs_to_reach_target_recall\"][f\"{float(r_target):.1f}\"][\"avg\"]\n",
    "                    reccomended_ipi = avg_dists / ipi_div\n",
    "                    recommended_mpi = avg_dists / mpi_div\n",
    "                    heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][k][r_target] = {\n",
    "                        \"ipi\": f\"{reccomended_ipi:.0f}\",\n",
    "                        \"mpi\": f\"{recommended_mpi:.0f}\"\n",
    "                    }\n",
    "        \n",
    "with open(f\"../../experiments/generated_json/heuristic_recommendations_params.json\", \"w\") as f:\n",
    "    json.dump(heuristic_recommendations_params, f, indent=4)\n",
    "\n",
    "# Print the recommendations in batch format\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        conf_str = f\"{ds_name}_tuples_k{k}=(\\n\"\n",
    "        for r_target in all_r_targets:\n",
    "            for ipi_div in ipi_div_factors:\n",
    "                for mpi_div in mpi_div_factors:\n",
    "                    if ipi_div > mpi_div:\n",
    "                        continue\n",
    "                    recommended_ipi = heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][k][r_target][\"ipi\"]\n",
    "                    recommended_mpi = heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][k][r_target][\"mpi\"]\n",
    "                    conf_str += f\"  \\\"{float(r_target):.2f} {recommended_ipi} {recommended_mpi}\\\" \\n\"\n",
    "        conf_str += \")\"\n",
    "        \n",
    "        print(conf_str, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare here all the versions of the recommendations\n",
    "heuristic_recommendations_params\n",
    "\n",
    "accepted_adaptive_heuristic_recommendations = {}\n",
    "accepted_static_heuristic_recommendations = {}\n",
    "\n",
    "#mpi_div_factors = [1, 2, 4, 8, 10, 16, 20]\n",
    "#ipi_div_factors =  [1, 2, 4, 8]\n",
    "ipi_div_factors = [2, 4]\n",
    "mpi_div_factors = [4, 8, 10]\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_k_values = [50]\n",
    "all_r_targets = [\"0.90\", \"0.99\"]\n",
    "\n",
    "# Calculate the pair of mpi-ipi that has the\n",
    "mpi_ipi_avg_speedups = {}\n",
    "for mpi_div in mpi_div_factors:\n",
    "    for ipi_div in ipi_div_factors:\n",
    "        if ipi_div > mpi_div:\n",
    "            continue\n",
    "        \n",
    "        ds_speedups = []\n",
    "        for ds_name in all_datasets:\n",
    "            M = dataset_params[ds_name][\"M\"]\n",
    "            efC = dataset_params[ds_name][\"efC\"]\n",
    "            efS = dataset_params[ds_name][\"efS\"]\n",
    "            ds_k_speedups = []\n",
    "            for k in all_k_values:\n",
    "                ds_k_r_speedups = []\n",
    "                for r_target in all_r_targets:\n",
    "                    mpi = heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][str(k)][r_target][\"mpi\"]\n",
    "                    ipi = heuristic_recommendations_params[f\"{ipi_div}-{mpi_div}\"][ds_name][str(k)][r_target][\"ipi\"]\n",
    "\n",
    "                    testing_df = pd.read_csv(get_interval_tuning_dataset_name(M, efC, efS, 1000, ds_name, k, float(r_target), ipi, mpi, \"heuristics\"))\n",
    "                    testing_df = testing_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",]]\n",
    "                    \n",
    "                    baseline_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, 1000, ds_name, k))\n",
    "                    baseline_df = baseline_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"]]\n",
    "                    \n",
    "                    speedup = baseline_df[\"elaps_ms\"].mean() / testing_df[\"elaps_ms\"].mean()\n",
    "                    \n",
    "                    ds_k_r_speedups.append(speedup)\n",
    "                ds_k_speedups.append(np.mean(ds_k_r_speedups))\n",
    "            ds_speedups.append(np.mean(ds_k_speedups))\n",
    "        \n",
    "        mpi_ipi_avg_speedups[f\"{ipi_div}-{mpi_div}\"] = np.mean(ds_speedups)\n",
    "                   \n",
    "# Sort by the mean speedup\n",
    "sorted_mpi_ipi_avg_speedups = {k: v for k, v in sorted(mpi_ipi_avg_speedups.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Print all nicely: ipi: XX mpi: XX speedup: XX\n",
    "for i, (ipi_mpi, speedup) in enumerate(sorted_mpi_ipi_avg_speedups.items()):\n",
    "    print(f\"(ipiDIV_mpiDIV)={ipi_mpi}: {speedup:.2f}x\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the parameters\n",
    "all_datasets = [\"T2I100M\", \"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.80\", \"0.85\" ,\"0.90\", \"0.95\", \"0.99\"]\n",
    "\n",
    "mpi_div_factor = 10\n",
    "ipi_div_factor = 2\n",
    "\n",
    "heuristic_recommendations_params = {}\n",
    "for ds_name in all_datasets:\n",
    "    heuristic_recommendations_params[ds_name] = {}\n",
    "    data_stat_filename = f\"../../experiments/generated_json/data_stats_{ds_name}.json\"\n",
    "    data_stats = {}\n",
    "    with open(data_stat_filename, \"r\") as f:\n",
    "        data_stats = json.load(f)\n",
    "    \n",
    "    for k in all_k_values:\n",
    "        \n",
    "        heuristic_recommendations_params[ds_name][k] = {}\n",
    "        conf_str = f\"{ds_name}_tuples_k{k}=(\\n\"\n",
    "        for r_target in all_r_targets:\n",
    "            avg_dists = data_stats[k][\"distance_calcs_to_reach_target_recall\"][f\"{float(r_target):.1f}\"][\"avg\"]\n",
    "            reccomended_ipi = avg_dists / ipi_div_factor\n",
    "            recommended_mpi = avg_dists / mpi_div_factor\n",
    "            conf_str += f\"  \\\"{float(r_target):.2f} {reccomended_ipi:.0f} {recommended_mpi:.0f}\\\" \\n\"\n",
    "            heuristic_recommendations_params[ds_name][k][r_target] = {\n",
    "                \"ipi\": f\"{reccomended_ipi:.0f}\",\n",
    "                \"mpi\": f\"{recommended_mpi:.0f}\"\n",
    "            }\n",
    "        \n",
    "        conf_str += \")\"\n",
    "        print(conf_str, \"\\n\") \n",
    "\n",
    "# Save the recommendations to a file\n",
    "mode = \"adaptive\"\n",
    "if mpi_div_factor == ipi_div_factor:\n",
    "    mode = \"static\"\n",
    "with open(f\"../../experiments/final_heuristic_{mode}_recommendations_params_ipidiv{ipi_div_factor}_mpidiv{mpi_div_factor}.json\", \"w\") as f:\n",
    "    json.dump(heuristic_recommendations_params, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Comparisons and Ablation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search results\n",
    "initial_prediction_intervals_normal = [400, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "min_prediction_intervals_normal = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900]\n",
    "\n",
    "initial_prediction_intervals_t2i = [400, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000]\n",
    "min_prediction_intervals_t2i = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500]\n",
    "\n",
    "target_recalls = [0.90, 0.99]\n",
    "accepted_tuning_confs = {}\n",
    "best_tuning_confs = {}\n",
    "\n",
    "all_datasets = [\"T2I100M\", \"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_k_values = [50]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    accepted_tuning_confs[ds_name] = {}\n",
    "    best_tuning_confs[ds_name] = {}\n",
    "    \n",
    "    M = dataset_params[ds_name][\"M\"]\n",
    "    efC = dataset_params[ds_name][\"efC\"]\n",
    "    efS = dataset_params[ds_name][\"efS\"]\n",
    "    \n",
    "    for k in all_k_values:\n",
    "        best_tuning_confs[ds_name][k] = {\n",
    "            \"r_targets\": [],\n",
    "            \"pi_s\": []\n",
    "        }\n",
    "                \n",
    "        accepted_tuning_confs[ds_name][k] = {}\n",
    "        \n",
    "        for target_recall in target_recalls:            \n",
    "            best_speedup = 0\n",
    "            best_mpi = 0\n",
    "            best_ipi = 0\n",
    "            best_target_recall = 0\n",
    "            best_num_queries_under_target_recall = 0\n",
    "            best_average_predictor_calls = 0\n",
    "            \n",
    "            if target_recall == 0.99 and ds_name == \"T2I100M\":\n",
    "                target_recall = 0.95\n",
    "            \n",
    "            accepted_tuning_confs[ds_name][k][target_recall] = []\n",
    "            \n",
    "            #normal_intervals = (initial_prediction_intervals_normal, min_prediction_intervals_normal)\n",
    "            #t2i_intervals = (initial_prediction_intervals_t2i, min_prediction_intervals_t2i)\n",
    "            #for intervals in [normal_intervals, t2i_intervals]:\n",
    "            \n",
    "            if ds_name == \"T2I100M\":\n",
    "                initial_prediction_intervals = initial_prediction_intervals_t2i\n",
    "                min_prediction_intervals = min_prediction_intervals_t2i\n",
    "            else:\n",
    "                initial_prediction_intervals = initial_prediction_intervals_normal\n",
    "                min_prediction_intervals = min_prediction_intervals_normal\n",
    "            \n",
    "            for ipi in initial_prediction_intervals:\n",
    "                for mpi in min_prediction_intervals:\n",
    "                    if (ipi < mpi):\n",
    "                        continue\n",
    "                                 \n",
    "                    testing_df = pd.read_csv(get_interval_tuning_dataset_name(M, efC, efS, 1000, ds_name, k, target_recall, ipi, mpi, \"search\"))\n",
    "                    testing_df = testing_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",]]\n",
    "                        \n",
    "                    baseline_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, 1000, ds_name, k))\n",
    "                    baseline_df = baseline_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"]]\n",
    "                        \n",
    "                    baseline_df = baseline_df[baseline_df[\"r\"] >= float(target_recall)]\n",
    "                    testing_df = testing_df[testing_df[\"qid\"].isin(baseline_df[\"qid\"])]\n",
    "                        \n",
    "                    avg_r_actual = testing_df[\"r_actual\"].mean()\n",
    "                    speedup = baseline_df[\"elaps_ms\"].mean() / testing_df[\"elaps_ms\"].mean()\n",
    "                        \n",
    "                    num_queries_under_target_recall = testing_df[testing_df[\"r_actual\"] < (target_recall - 0.01)].shape[0]\n",
    "                    average_predictor_calls = testing_df[\"r_predictor_calls\"].mean()\n",
    "                                            \n",
    "                    if speedup > best_speedup and avg_r_actual >= target_recall:\n",
    "                        best_speedup = speedup\n",
    "                        best_ipi = ipi\n",
    "                        best_mpi = mpi\n",
    "                        best_target_recall = avg_r_actual\n",
    "                        best_num_queries_under_target_recall = num_queries_under_target_recall\n",
    "                        best_average_predictor_calls = average_predictor_calls\n",
    "                        \n",
    "                    if avg_r_actual >= target_recall - 0.01 and speedup >= 1.0:\n",
    "                        entry = {\n",
    "                                \"speedup\": speedup,\n",
    "                                \"ipi\": ipi,\n",
    "                                \"mpi\": mpi,\n",
    "                                \"recall\": avg_r_actual,\n",
    "                                \"rqut\": num_queries_under_target_recall,\n",
    "                                \"time\": testing_df[\"elaps_ms\"].mean(),\n",
    "                        }\n",
    "                        accepted_tuning_confs[ds_name][k][target_recall].append(entry)\n",
    "                        #else:\n",
    "                            #print(f\"        Discarded tuning conf IPI,MPI: ({ipi},{mpi}) that reached {avg_r_actual:.2f} recall with speedup {speedup:.2f}x\")\n",
    "                        #    pass\n",
    "                        #results[(ds_name, k)].append((ipi, mpi, avg_r_actual, speedup))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            #print(f\"    Rt={target_recall:.2f}: Speedup is {best_speedup:.2f}x with IPI={best_ipi}, MPI={best_mpi}, recall={best_target_recall:.2f}, QUT={best_num_queries_under_target_recall}, AVG Calls={best_average_predictor_calls}\")     \n",
    "            \n",
    "            best_tuning_confs[ds_name][k][\"r_targets\"].append(target_recall)\n",
    "            best_tuning_confs[ds_name][k][\"pi_s\"].append([best_ipi, best_mpi])\n",
    "    print()       \n",
    "\n",
    "sorted_accepted_tuning_confs = {}\n",
    "for ds_name in accepted_tuning_confs.keys():\n",
    "    sorted_accepted_tuning_confs[ds_name] = {}\n",
    "    for k in accepted_tuning_confs[ds_name].keys():\n",
    "        sorted_accepted_tuning_confs[ds_name][k] = {}\n",
    "        for target_recall in accepted_tuning_confs[ds_name][k].keys():\n",
    "            if target_recall == 0.99 and ds_name == \"T2I100M\":\n",
    "                target_recall = 0.95\n",
    "            sorted_accepted_tuning_confs[ds_name][k][target_recall] = sorted(accepted_tuning_confs[ds_name][k][target_recall], key=lambda x: x[\"speedup\"], reverse=True)\n",
    "\n",
    "DARTH_search_confs = {}\n",
    "for ds_name in all_datasets:\n",
    "    DARTH_search_confs[ds_name] = {}\n",
    "    for k in all_k_values:\n",
    "        DARTH_search_confs[ds_name][k] = {}\n",
    "        print(f\"{ds_name}, k={k}\")\n",
    "        for target_recall in target_recalls:\n",
    "            if target_recall == 0.99 and ds_name == \"T2I100M\":\n",
    "                target_recall = 0.95\n",
    "            entry = sorted_accepted_tuning_confs[ds_name][k][target_recall][0]\n",
    "            DARTH_search_confs[ds_name][k][target_recall] = entry\n",
    "            print(f\"    Rt={target_recall:.2f} => Speedup: {entry['speedup']:.2f}x | IPI: {entry['ipi']} | MPI: {entry['mpi']} | Recall: {entry['recall']:.2f} | QUT: {entry['rqut']}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall, speedups etc for the static prediction interval method\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [50]\n",
    "target_recalls = [0.90, 0.99]\n",
    "prediction_intervals_normal = [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "prediction_intervals_t2i = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500, 10000]\n",
    "\n",
    "static_prediction_interval_results = {}\n",
    "for ds_name in all_datasets:\n",
    "    static_prediction_interval_results[ds_name] = {}\n",
    "    M = dataset_params[ds_name][\"M\"]\n",
    "    efC = dataset_params[ds_name][\"efC\"]\n",
    "    efS = dataset_params[ds_name][\"efS\"]\n",
    "    for k in all_k_values:\n",
    "        static_prediction_interval_results[ds_name][k] = {}\n",
    "        for target_recall in target_recalls:\n",
    "            if ds_name == \"T2I100M\" and target_recall == 0.99:\n",
    "                target_recall = 0.95\n",
    "            \n",
    "            static_prediction_interval_results[ds_name][k][target_recall] = []\n",
    "            \n",
    "            prediction_intervals = prediction_intervals_normal\n",
    "            if ds_name == \"T2I100M\":\n",
    "                prediction_intervals = prediction_intervals_t2i\n",
    "            \n",
    "            for pi in prediction_intervals:\n",
    "                mpi, ipi = pi, pi\n",
    " \n",
    "                testing_df = pd.read_csv(get_interval_tuning_dataset_name(M, efC, efS, 1000, ds_name, k, target_recall, ipi, mpi, \"static\"))\n",
    "                testing_df = testing_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",]]\n",
    "                    \n",
    "                baseline_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, 1000, ds_name, k))\n",
    "                baseline_df = baseline_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"]]\n",
    "                \n",
    "                baseline_df = baseline_df[baseline_df[\"r\"] >= float(target_recall)]\n",
    "                testing_df = testing_df[testing_df[\"qid\"].isin(baseline_df[\"qid\"])]\n",
    "                    \n",
    "                avg_r_actual = testing_df[\"r_actual\"].mean()\n",
    "                speedup = baseline_df[\"elaps_ms\"].mean() / testing_df[\"elaps_ms\"].mean()\n",
    "                    \n",
    "                num_queries_under_target_recall = testing_df[testing_df[\"r_actual\"] < (target_recall - 0.01)].shape[0]\n",
    "                average_predictor_calls = testing_df[\"r_predictor_calls\"].mean()\n",
    "                    \n",
    "                if avg_r_actual >= target_recall - 0.01:\n",
    "                    entry = {\n",
    "                        \"speedup\": speedup,\n",
    "                        \"ipi\": ipi,\n",
    "                        \"mpi\": mpi,\n",
    "                        \"recall\": avg_r_actual,\n",
    "                        \"rqut\": num_queries_under_target_recall,\n",
    "                        \"time\": testing_df[\"elaps_ms\"].mean()\n",
    "                    }\n",
    "                    static_prediction_interval_results[ds_name][k][target_recall].append(entry)\n",
    "\n",
    "# Sort the results\n",
    "sorted_static_prediction_interval_results = {}\n",
    "for ds_name in static_prediction_interval_results.keys():\n",
    "    sorted_static_prediction_interval_results[ds_name] = {}\n",
    "    for k in static_prediction_interval_results[ds_name].keys():\n",
    "        sorted_static_prediction_interval_results[ds_name][k] = {}\n",
    "        for target_recall in static_prediction_interval_results[ds_name][k].keys():\n",
    "            if ds_name == \"T2I100M\" and target_recall == 0.99:\n",
    "                target_recall = 0.95\n",
    "            sorted_static_prediction_interval_results[ds_name][k][target_recall] = sorted(static_prediction_interval_results[ds_name][k][target_recall], key=lambda x: x[\"speedup\"], reverse=True)\n",
    "                    \n",
    "# Print the best results\n",
    "DARTH_static_confs = {}\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        print(f\"{ds_name}, k={k}\")\n",
    "        for target_recall in target_recalls:\n",
    "            if ds_name == \"T2I100M\" and target_recall == 0.99:\n",
    "                target_recall = 0.95\n",
    "            entry = sorted_static_prediction_interval_results[ds_name][k][target_recall][0]\n",
    "            print(f\"    Rt={target_recall:.2f} => Speedup: {entry['speedup']:.2f}x | IPI: {entry['ipi']} | MPI: {entry['mpi']} | Recall: {entry['recall']:.2f} | QUT: {entry['rqut']}\")\n",
    "            DARTH_static_confs[(ds_name, k, target_recall)] = entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the resutls ffrom the heuristic recommendations\n",
    "DARTH_heuristic_adaptive_confs = {}\n",
    "DARTH_heuristic_static_confs = {}\n",
    "\n",
    "heuristic_adaptive_filename = \"../../experiments/generated_json/final_heuristic_adaptive_recommendations_params_ipidiv2_mpidiv10.json\"\n",
    "heuristic_static_filename = \"../../experiments/generated_json/final_heuristic_static_recommendations_params_ipidiv4_mpidiv4.json\"\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [50]\n",
    "target_recalls = [0.90, 0.99]\n",
    "\n",
    "confs = [DARTH_heuristic_adaptive_confs, DARTH_heuristic_static_confs]\n",
    "filenames = [heuristic_adaptive_filename, heuristic_static_filename]\n",
    "modes = [\"adaptive\", \"static\"]\n",
    "\n",
    "for DARTH_conf, filename, mode in zip(confs, filenames, modes):\n",
    "    heuristic_recommendations_params = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        heuristic_recommendations_params = json.load(f)\n",
    "\n",
    "    print(f\"Processing {mode} recommendations\")\n",
    "    \n",
    "    for ds_name in all_datasets:\n",
    "        DARTH_conf[ds_name] = {}\n",
    "        M = dataset_params[ds_name][\"M\"]\n",
    "        efC = dataset_params[ds_name][\"efC\"]\n",
    "        efS = dataset_params[ds_name][\"efS\"]\n",
    "        for k in all_k_values:\n",
    "            print(f\"    {ds_name}, k={k}\")\n",
    "            DARTH_conf[ds_name][k] = {}\n",
    "            for target_recall in target_recalls:\n",
    "                if ds_name == \"T2I100M\" and target_recall == 0.99:\n",
    "                    target_recall = 0.95\n",
    "                \n",
    "                mpi = heuristic_recommendations_params[ds_name][str(k)][str(f\"{float(target_recall):.2f}\")][\"mpi\"]\n",
    "                ipi = heuristic_recommendations_params[ds_name][str(k)][str(f\"{float(target_recall):.2f}\")][\"ipi\"]\n",
    "                \n",
    "                print(f\"        Rt={target_recall:.2f} => IPI: {ipi} | MPI: {mpi}\")\n",
    "                \n",
    "                testing_df = pd.read_csv(get_interval_tuning_dataset_name(M, efC, efS, 1000, ds_name, k, target_recall, ipi, mpi, \"heuristics\"))\n",
    "                testing_df = testing_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r_actual\", \"r_predicted\", \"r_predictor_calls\", \"r_predictor_time_ms\",]]\n",
    "                \n",
    "                baseline_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, 1000, ds_name, k))\n",
    "                baseline_df = baseline_df[[\"qid\", \"step\", \"dists\", \"inserts\", \"elaps_ms\", \"r\"]]\n",
    "                \n",
    "                baseline_df = baseline_df[baseline_df[\"r\"] >= float(target_recall)]\n",
    "                testing_df = testing_df[testing_df[\"qid\"].isin(baseline_df[\"qid\"])]\n",
    "                \n",
    "                avg_r_actual = testing_df[\"r_actual\"].mean()\n",
    "                speedup = baseline_df[\"elaps_ms\"].mean() / testing_df[\"elaps_ms\"].mean()\n",
    "                \n",
    "                num_queries_under_target_recall = testing_df[testing_df[\"r_actual\"] < (target_recall - 0.01)].shape[0]\n",
    "                average_predictor_calls = testing_df[\"r_predictor_calls\"].mean()\n",
    "                                \n",
    "                DARTH_conf[ds_name][k][target_recall] = {\n",
    "                    \"speedup\": speedup,\n",
    "                    \"ipi\": int(ipi),\n",
    "                    \"mpi\": int(mpi),\n",
    "                    \"recall\": avg_r_actual,\n",
    "                    \"rqut\": num_queries_under_target_recall,\n",
    "                    \"time\": testing_df[\"elaps_ms\"].mean()\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DARTH_heuristic_adaptive_confs[\"T2I100M\"][k][0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "all_datasets = [\"SIFT100M\", \"DEEP100M\", \"GLOVE100\", \"GIST1M\"]\n",
    "all_k_values = [50]\n",
    "\n",
    "algos = {\n",
    "    \"Static-Grid-Search\": {\n",
    "        \"color\": \"peachpuff\",\n",
    "    },\n",
    "    \"Adaptive-Grid-Search\": {\n",
    "        \"color\": \"darksalmon\",\n",
    "    },\n",
    "    \"Static-Heuristic\": {\n",
    "        \"color\": \"burlywood\",\n",
    "    },\n",
    "    \"Adaptive-Heuristic\": {\n",
    "        \"color\": \"tomato\",\n",
    "    }\n",
    "}\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 41})\n",
    "\n",
    "fig_legend, ax_legend = plt.subplots(figsize=(8, 1))  # Adjust the figsize for better alignment\n",
    "legend_elements = [\n",
    "    Patch(facecolor=algos[\"Static-Grid-Search\"][\"color\"], edgecolor=algos[\"Static-Grid-Search\"][\"color\"], label=\"St-GS\"),\n",
    "    Patch(facecolor=algos[\"Adaptive-Grid-Search\"][\"color\"], edgecolor=algos[\"Adaptive-Grid-Search\"][\"color\"], label=\"Ad-GS\"),\n",
    "    Patch(facecolor=algos[\"Static-Heuristic\"][\"color\"], edgecolor=algos[\"Static-Heuristic\"][\"color\"], label=\"St-Heur\"),\n",
    "    Patch(facecolor=algos[\"Adaptive-Heuristic\"][\"color\"], edgecolor=algos[\"Adaptive-Heuristic\"][\"color\"], label=\"Ad-Heur\"),\n",
    "]\n",
    "#ax_legend.legend(handles=legend_elements, loc='center', ncol=4, frameon=False)\n",
    "ax_legend.legend(\n",
    "    handles=legend_elements, \n",
    "    loc='center', \n",
    "    ncol=4, \n",
    "    frameon=False,\n",
    "    handletextpad=0.3,  # Adjust padding between handle and text\n",
    "    columnspacing=0.5,  # Adjust space between columns\n",
    "    labelspacing=0.3    # Adjust vertical space between labels\n",
    ")\n",
    "ax_legend.axis('off')\n",
    "fig_legend.savefig(f\"{PLOTS_DIR}interval_ablation_legend_only.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "#plt.rcParams.update({\"font.size\": 36})  \n",
    "\n",
    "measures = {\n",
    "    \"speedup\": {\n",
    "        \"title\": \"Speedup\",\n",
    "        \"label\": \"Times Faster\",\n",
    "        \"y-bottom-lim\": 1\n",
    "    },\n",
    "    #\"time\": {\n",
    "    #    \"title\": \"Time\",\n",
    "    #    \"label\": \"Search Time (ms)\",\n",
    "    #    \"y-bottom-lim\": 0\n",
    "    #}\n",
    "    #\"rqut\": {\n",
    "    #    \"title\": \"Queries% Under Target Recall\",\n",
    "    #    \"label\": \"RQUT\",\n",
    "    #    \"y-bottom-lim\": 0\n",
    "    #    \n",
    "    #}\n",
    "}\n",
    "\n",
    "#change colors\n",
    "target_recalls = [0.90, 0.99]\n",
    "\n",
    "# Calculate also the average speedup of grid search adaptive to compare with the heuristic adaptive\n",
    "avg_speedup_grid_search_adaptive_over_heuristic = 0\n",
    "avg_speedup_grid_search_adaptive_over_static = 0\n",
    "avg_speedup_heuristic_adaptive_over_static_heuristic = 0\n",
    "speedup_nums = 0\n",
    "\n",
    "max_speedup_grid_search_adaptive_over_static = 0\n",
    "max_speedup_heuristic_adaptive_over_static = 0\n",
    "\n",
    "for k in all_k_values:\n",
    "    for i, target_recall in enumerate(target_recalls):\n",
    "        for measure, measure_info in measures.items():\n",
    "            DARTH_heuristic_adaptive = []#[DARTH_heuristic_adaptive_confs[ds_name][k][target_recall][measure] for ds_name in all_datasets]\n",
    "            DARTH_heuristic_static = []#[DARTH_heuristic_static_confs[ds_name][k][target_recall][measure] for ds_name in all_datasets]\n",
    "            DARTH_grid_search_adaptive = []#[DARTH_search_confs[ds_name][k][target_recall][measure] for ds_name in all_datasets]\n",
    "            DARTH_grid_search_static = []#[DARTH_static_confs[(ds_name, k, target_recall)][measure] for ds_name in all_datasets]\n",
    "            for ds_name in all_datasets:\n",
    "                prev_recall = target_recall\n",
    "                if ds_name == \"T2I100M\" and target_recall == 0.99:\n",
    "                    target_recall = 0.95\n",
    "                \n",
    "                DARTH_heuristic_adaptive.append(DARTH_heuristic_adaptive_confs[ds_name][k][target_recall][measure])\n",
    "                DARTH_heuristic_static.append(DARTH_heuristic_static_confs[ds_name][k][target_recall][measure])\n",
    "                DARTH_grid_search_adaptive.append(DARTH_search_confs[ds_name][k][target_recall][measure])\n",
    "                DARTH_grid_search_static.append(DARTH_static_confs[(ds_name, k, target_recall)][measure])\n",
    "\n",
    "                time_heuristic_adaptive = DARTH_heuristic_adaptive_confs[ds_name][k][target_recall][\"time\"]\n",
    "                time_grid_search_adaptive = DARTH_search_confs[ds_name][k][target_recall][\"time\"]\n",
    "                time_heuristic_static = DARTH_heuristic_static_confs[ds_name][k][target_recall][\"time\"]\n",
    "                time_grid_search_static = DARTH_static_confs[(ds_name, k, target_recall)][\"time\"]\n",
    "                \n",
    "                avg_speedup_grid_search_adaptive_over_heuristic += time_heuristic_adaptive / time_grid_search_adaptive\n",
    "                avg_speedup_grid_search_adaptive_over_static += time_heuristic_static / time_grid_search_adaptive\n",
    "                avg_speedup_heuristic_adaptive_over_static_heuristic += time_heuristic_static / time_heuristic_adaptive\n",
    "                \n",
    "                speedup_nums += 1\n",
    "                \n",
    "                speedup_grid_search_adaptive_vs_static = time_grid_search_static / time_grid_search_adaptive\n",
    "                if speedup_grid_search_adaptive_vs_static > max_speedup_grid_search_adaptive_over_static:\n",
    "                    max_speedup_grid_search_adaptive_over_static = speedup_grid_search_adaptive_vs_static\n",
    "                    \n",
    "                speedup_heuristic_adaptive_vs_static = time_heuristic_static / time_heuristic_adaptive\n",
    "                if speedup_heuristic_adaptive_vs_static > max_speedup_heuristic_adaptive_over_static:\n",
    "                    max_speedup_heuristic_adaptive_over_static = speedup_heuristic_adaptive_vs_static\n",
    "                \n",
    "                target_recall = prev_recall\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "            \n",
    "            x = np.arange(len(all_datasets))\n",
    "            width = 0.2\n",
    "            \n",
    "            ax.bar(x - 1.5 * width, DARTH_grid_search_static, width, label=\"Static-Grid-Search\", alpha=0.8, color=algos[\"Static-Grid-Search\"][\"color\"], edgecolor=\"black\", linewidth=2)\n",
    "            ax.bar(x - 0.5 * width, DARTH_grid_search_adaptive, width, label=\"Adaptive-Grid-Search\", alpha=0.8, color=algos[\"Adaptive-Grid-Search\"][\"color\"], edgecolor=\"black\", linewidth=2)\n",
    "            ax.bar(x + 0.5 * width, DARTH_heuristic_static, width, label=\"Static-Heuristic\", alpha=0.8,color=algos[\"Static-Heuristic\"][\"color\"], edgecolor=\"black\", linewidth=2)\n",
    "            ax.bar(x + 1.5 * width, DARTH_heuristic_adaptive, width, label=\"Adaptive-Heuristic\", alpha=0.8, color=algos[\"Adaptive-Heuristic\"][\"color\"], edgecolor=\"black\", linewidth=2)\n",
    "            \n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(dataset_params[ds_namee][\"label\"] for ds_namee in all_datasets)\n",
    "            \n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_visible(False)\n",
    "                \n",
    "            # Turn x-axis labels to 45 degrees\n",
    "            plt.xticks(rotation=15)\n",
    "            #plt.yticks(fontsize=36)\n",
    "            \n",
    "            # Make the font size of x-axis labels smaller\n",
    "            plt.xticks(fontsize=36)\n",
    "            #plt.yticks(fontsize=30)\n",
    "            # make ylabel smaller\n",
    "            \n",
    "            \n",
    "            # change y name font size \n",
    "            ax.set_ylabel(f\"{measure_info['label']}\")#, fontsize=28)\n",
    "            ax.grid(alpha=0.8, axis=\"y\", linestyle=\"--\", linewidth=1.0)\n",
    "            ax.set_ylim(bottom=measure_info[\"y-bottom-lim\"])\n",
    "        \n",
    "            fig.tight_layout()\n",
    "            fig.savefig(f\"{PLOTS_DIR}interval_ablation_{measure}_k{k}_rt{target_recall:.2f}.pdf\", bbox_inches=\"tight\")\n",
    "            print(f\"Saved plot at {PLOTS_DIR}interval_ablation_{measure}_k{k}_rt{target_recall:.2f}.pdf\")\n",
    "            plt.show()\n",
    "\n",
    "avg_speedup_grid_search_adaptive_over_heuristic /= speedup_nums\n",
    "avg_speedup_grid_search_adaptive_over_static /= speedup_nums\n",
    "avg_speedup_heuristic_adaptive_over_static_heuristic /= speedup_nums\n",
    "\n",
    "print(f\"Average speedup of Grid-Search Adaptive over Heuristic Adaptive: {avg_speedup_grid_search_adaptive_over_heuristic:.2f}x\")\n",
    "print(f\"Average speedup of Grid-Search Adaptive over Static: {avg_speedup_grid_search_adaptive_over_static:.2f}x\")\n",
    "print(f\"Average speedup of Heuristic Adaptive over Static Heuristic: {avg_speedup_heuristic_adaptive_over_static_heuristic:.2f}x\")\n",
    "\n",
    "print(f\"Max speedup of Grid-Search Adaptive over Static: {max_speedup_grid_search_adaptive_over_static:.2f}x\")\n",
    "print(f\"Max speedup of Heuristic Adaptive over Static: {max_speedup_heuristic_adaptive_over_static:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned parameter organization for LAET and Classic HNSW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tuned multipliers of LAET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laet_dataset_F_params = {\n",
    "    \"SIFT100M\": 241,\n",
    "    \"GLOVE100\": 200,\n",
    "    \"GIST1M\": 1260,\n",
    "    \"DEEP100M\": 368,\n",
    "    \"T2I100M\": 400,\n",
    "}\n",
    "\n",
    "laet_multipliers_conf = {}\n",
    "\n",
    "with open(\"../../experiments/generated_json/laet_tuning_results_memoryFalse_validationSize1000.json\", \"r\") as f:\n",
    "    laet_multipliers_conf = json.load(f)\n",
    "\n",
    "all_datasets = [\"T2I100M\",\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        str_conf = f\"{ds_name}_tuples_k{k}=(\\n\"\n",
    "        for r_target in all_r_targets:\n",
    "            F = laet_dataset_F_params[ds_name]\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            multiplier_info = laet_multipliers_conf[ds_name][k][r_target] # Make sure to see if this is correct\n",
    "            multiplier = multiplier_info[\"min_m\"]\n",
    "            str_conf += f\"  \\\"{float(r_target):.2f} {F} {float(multiplier):.2f}\\\" \\n\"\n",
    "        str_conf += \")\"\n",
    "        print(str_conf, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate tuned multipliers of Classic HNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_hnsw_conf = {}\n",
    "with open(\"../../experiments/generated_json/classic_hnsw_tuning_results_memoryFalse_validationSize1000.json\", \"r\") as f:\n",
    "    classic_hnsw_conf = json.load(f)\n",
    "    \n",
    "all_datasets = [\"T2I100M\",\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "all_r_targets = [\"0.8\", \"0.85\", \"0.9\", \"0.95\", \"0.99\"]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    for k in all_k_values:\n",
    "        str_conf = f\"{ds_name}_tuples_k{k}=(\\n\"\n",
    "        for r_target in all_r_targets:\n",
    "            if ds_name == \"T2I100M\" and r_target == \"0.99\":\n",
    "                continue\n",
    "            efS = classic_hnsw_conf[ds_name][k][r_target][\"min_efS\"]\n",
    "            str_conf += f\"  \\\"{float(r_target):.2f} {efS}\\\" \\n\"\n",
    "        str_conf += \")\"\n",
    "        print(str_conf, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [\"SIFT100M\", \"GLOVE100\", \"GIST1M\", \"DEEP100M\", \"T2I100M\"]\n",
    "all_k_values = [\"10\", \"25\", \"50\", \"75\", \"100\"]\n",
    "\n",
    "for ds_name in all_datasets:\n",
    "    M = dataset_params[ds_name][\"M\"]\n",
    "    efC = dataset_params[ds_name][\"efC\"]\n",
    "    efS = dataset_params[ds_name][\"efS\"]\n",
    "    \n",
    "    print(f\"{ds_name}\")\n",
    "    \n",
    "    avg_recall_k = []\n",
    "    for k in all_k_values:        \n",
    "        no_early_termination_df = pd.read_csv(get_no_early_stop_dataset_name(M, efC, efS, 1000, ds_name, k))\n",
    "        recall = no_early_termination_df[\"r\"].mean()\n",
    "        avg_recall_k.append(recall)\n",
    "        print(f\"        k={k} => {recall:.3f}\")\n",
    "    \n",
    "    print(f\"    Avg Recall: {np.mean(avg_recall_k):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "localPyLibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
